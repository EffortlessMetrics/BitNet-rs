# BitNet.rs Correction Policy - Sample Configuration
# ==================================================
#
# This file demonstrates policy-driven corrections for known-bad GGUF models.
# Use this as a template for creating model-specific correction policies.
#
# **IMPORTANT**: This is a TEMPORARY WORKAROUND. Always prefer regenerating
# GGUF files with proper weight formats for production use.
#
# Usage:
#   export BITNET_CORRECTION_POLICY=./config/correction-policy.yml
#   export BITNET_ALLOW_RUNTIME_CORRECTIONS=1  # Required for corrections to apply
#   cargo run -p bitnet-cli --no-default-features --features cpu -- run \
#     --model path/to/model.gguf --prompt "Test"
#
# Security posture:
# - Strict mode (BITNET_STRICT_MODE=1) disables ALL corrections
# - Each correction is tied to a specific model fingerprint (SHA256)
# - All corrections generate receipts in the model load report
# - CI blocks BITNET_ALLOW_RUNTIME_CORRECTIONS=1 to prevent production deployment
#
# See: docs/explanation/correction-policy.md for complete documentation

version: 1

models:
  # Example 1: I2_S Dequantization Override (inverted scales)
  # ---------------------------------------------------------
  # Use when Q/K/V projection RMS values are orders of magnitude wrong.
  # Symptoms:
  #   - Q/K/V RMS with inv=false: O(10^5) (catastrophic)
  #   - Q/K/V RMS with inv=true: O(10^3) (healthy, matches FFN)
  #   - Attention scores explode, outputs are gibberish
  #
  # To diagnose:
  #   RUST_LOG=info cargo run -p bitnet-cli -- run --model model.gguf
  #   Look for "PROJ load:" logs showing RMS values

  - fingerprint: "sha256-0000000000000000000000000000000000000000000000000000000000000000"
    notes: |
      BitNet 2B 4T GGUF with inverted I2_S quantization scales in Q/K/V projections.
      Measured RMS:
        - Q/K/V with inv=false: ~100,000 (wrong)
        - Q/K/V with inv=true: ~800 (correct, matches FFN)
        - FFN gate/up/down: ~850 (baseline)

      Fix: Use inv=false (do NOT invert scales - they are correct as-is)
      Note: Despite the name, inv=false often means "scales are already correct"
    corrections:
      # Override I2_S dequantization for attention projections
      - type: I2S_DEQUANT_OVERRIDE
        tensors:
          - "attn_q.weight"    # BitNet-style naming
          - "attn_k.weight"
          - "attn_v.weight"
          - "q_proj.weight"    # LLaMA-style naming
          - "k_proj.weight"
          - "v_proj.weight"
        inv: false   # Use inv=false when scales are correct (no inversion needed)
        k: 1.0       # Multiplicative factor (typically 1.0)

  # Example 2: LayerNorm Gamma Rescale (quantized LN weights)
  # ----------------------------------------------------------
  # Use when LayerNorm gamma RMS is far from ~1.0 (typically ~0.01-0.02).
  # Symptoms:
  #   - Validation warnings: "suspicious LayerNorm gamma"
  #   - LayerNorm gamma RMS ~0.018 across all layers
  #   - Inference outputs gibberish or tied logits
  #   - Model fails in strict mode (BITNET_STRICT_MODE=1)
  #
  # Root cause:
  #   - LayerNorm weights were quantized to I2_S (should be FP16/FP32)
  #
  # To diagnose:
  #   cargo run -p bitnet-cli -- inspect --ln-stats path/to/model.gguf

  - fingerprint: "sha256-1111111111111111111111111111111111111111111111111111111111111111"
    notes: |
      GGUF with incorrectly quantized LayerNorm gamma weights.
      Measured LayerNorm gamma RMS: ~0.018 (should be ~1.0)
      All 24 layers affected.

      LayerNorm should ALWAYS be FP16/FP32, never quantized.
      This policy rescales corrupted weights as a temporary workaround.

      Proper fix: Regenerate GGUF with --skip-layernorm-quantization flag.
    corrections:
      - type: LN_GAMMA_RESCALE_RMS
        target_rms: 1.0          # Target RMS for LayerNorm gamma (typically 1.0)
        clamp: [0.01, 100.0]     # Safety clamp: factor ∈ [1%, 10000%]

  # Example 3: Combined LayerNorm + I2_S Corrections
  # -------------------------------------------------
  # For models with BOTH LayerNorm and I2_S issues.

  - fingerprint: "sha256-2222222222222222222222222222222222222222222222222222222222222222"
    notes: |
      Custom BitNet model with multiple issues:
        1. Quantized LayerNorm weights (RMS ~0.02)
        2. Correct I2_S scales for attention (inv=false)

      This policy applies both corrections simultaneously.
    corrections:
      # Fix LayerNorm gamma RMS
      - type: LN_GAMMA_RESCALE_RMS
        target_rms: 1.0
        clamp: [0.01, 100.0]

      # Override I2_S for attention (even though scales look OK)
      - type: I2S_DEQUANT_OVERRIDE
        tensors:
          - "attn_q.weight"
          - "attn_k.weight"
          - "attn_v.weight"
        inv: false
        k: 1.0

# ==============================================================================
# How to Create a Policy Entry for Your Model
# ==============================================================================
#
# 1. Get your model's fingerprint:
#    RUST_LOG=info cargo run -p bitnet-cli -- run --model path/to/model.gguf
#    Look for: "INFO Model fingerprint: sha256-abc123..."
#
# 2. Diagnose the issue:
#    a) LayerNorm issues:
#       cargo run -p bitnet-cli -- inspect --ln-stats model.gguf
#       If gamma RMS is far from 1.0 (e.g., ~0.01-0.02), use LN_GAMMA_RESCALE_RMS
#
#    b) I2_S issues:
#       RUST_LOG=info cargo run -- run --model model.gguf
#       Look for "PROJ load:" logs. If Q/K/V RMS differs from FFN by >10×, try I2S_DEQUANT_OVERRIDE
#
# 3. Add policy entry:
#    - fingerprint: "sha256-YOUR_FINGERPRINT_HERE"
#      notes: "Description of issue and measurements"
#      corrections:
#        - type: LN_GAMMA_RESCALE_RMS  # or I2S_DEQUANT_OVERRIDE
#          # ... parameters ...
#
# 4. Test:
#    export BITNET_CORRECTION_POLICY=./config/correction-policy.yml
#    export BITNET_ALLOW_RUNTIME_CORRECTIONS=1
#    cargo run -p bitnet-cli -- run --model model.gguf --prompt "Test"
#
# 5. Verify receipts in logs:
#    Look for "Applied N corrections during model load" with correction details
#
# ==============================================================================
# Migration Path: From Corrections to Clean GGUF
# ==============================================================================
#
# Corrections are TEMPORARY. Follow this workflow:
#
# 1. **Diagnose**: Use `bitnet inspect --ln-stats` to identify issues
# 2. **Policy**: Create policy entry with model fingerprint
# 3. **Verify**: Test corrections with BITNET_ALLOW_RUNTIME_CORRECTIONS=1
# 4. **Regenerate**: Create new GGUF with proper formats:
#      - LayerNorm: Use --skip-layernorm-quantization or equivalent
#      - I2_S: Ensure scales are computed correctly during export
# 5. **Validate**: Compare corrected vs clean GGUF inference outputs
# 6. **Retire**: Remove policy entry once clean GGUF is deployed
#
# CI blocks BITNET_ALLOW_RUNTIME_CORRECTIONS=1 to prevent production use.
#
# ==============================================================================
