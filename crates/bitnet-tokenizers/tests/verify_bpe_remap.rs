//! Verification test for BPE piece‚ÜíGGUF-ID remapping
//!
//! This test verifies that the fix for tokenizer parity is working correctly.
//! It checks that "What is 2+2?" produces the golden token IDs matching llama.cpp.

use std::fs;

#[test]
fn verify_bpe_first_token_remap() {
    // Use TOKENIZER_TEST_MODEL or BITNET_GGUF
    let model_path = std::env::var("TOKENIZER_TEST_MODEL")
        .or_else(|_| std::env::var("BITNET_GGUF"))
        .or_else(|_| std::env::var("CROSSVAL_GGUF"))
        .unwrap_or_else(|_| {
            // Try default location
            "models/microsoft-bitnet-b1.58-2B-4T-gguf/ggml-model-i2_s.gguf".to_string()
        });

    // Check if model exists
    if !std::path::Path::new(&model_path).exists() {
        eprintln!("‚ö†Ô∏è  Skipping test: Model not found at {}", model_path);
        eprintln!("   Set TOKENIZER_TEST_MODEL or BITNET_GGUF to a valid GGUF model path");
        return;
    }

    let data = fs::read(&model_path).expect("Failed to read model");

    // Load tokenizer using RustTokenizer
    use bitnet_models::formats::gguf::GgufReader;
    use bitnet_tokenizers::RustTokenizer;

    let reader = GgufReader::new(&data).expect("Failed to parse GGUF");

    // Check if this is a GPT-2 tokenizer
    let tok_model = reader.get_string_metadata("tokenizer.ggml.model");
    if tok_model.as_deref() != Some("gpt2") {
        eprintln!("‚ö†Ô∏è  Skipping test: Model has tokenizer kind {:?}, not gpt2", tok_model);
        return;
    }

    let tokenizer = RustTokenizer::from_gguf(&reader).expect("Failed to load tokenizer");

    // Test encoding without BOS (to match golden tokens)
    let text = "What is 2+2?";
    let ids = tokenizer.encode(text, false, false).expect("Encoding failed");

    println!("üìù Text: {:?}", text);
    println!("üî¢ Token IDs: {:?}", ids);
    println!("üéØ First token ID: {}", ids.first().unwrap());
    println!();

    // Look up expected first token from GGUF vocab (model-specific)
    // For GPT-2 BPE with add_prefix_space, "What" should be " What" (with leading space)
    let tokens = reader
        .get_string_array_metadata("tokenizer.ggml.tokens")
        .expect("Missing tokenizer.ggml.tokens");

    // Find the token for " What" (with ƒ† or literal space)
    let expected_first = tokens
        .iter()
        .position(|t| t == "ƒ†What" || t == " What")
        .expect("Token 'ƒ†What' or ' What' not found in GGUF vocab") as u32;

    println!("üîç Expected first token ID from GGUF vocab: {} ('ƒ†What' or ' What')", expected_first);

    // Check first token (critical for parity)
    assert_eq!(
        ids.first().copied(),
        Some(expected_first),
        "First token ID mismatch: got {}, expected {} from GGUF vocab",
        ids.first().unwrap(),
        expected_first
    );

    println!("‚úÖ First token ID matches GGUF vocab ({})", expected_first);
    println!("   This confirms the BPE piece‚ÜíGGUF-ID remapping is working correctly!");
    println!();

    // Verify that we're using the GGUF token ID, not HuggingFace piece ID
    println!("‚úÖ Test passed: BPE remap uses model-specific GGUF token IDs");
}
