{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BitNet Python to bitnet_py Migration - Basic Example\n",
    "\n",
    "This notebook demonstrates the basic migration process from the original BitNet Python implementation to the new Rust-based `bitnet_py` library.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The migration process involves:\n",
    "1. Installing bitnet_py\n",
    "2. Updating imports\n",
    "3. Adapting configuration\n",
    "4. Testing compatibility\n",
    "5. Performance comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Installation and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install bitnet_py (uncomment if not already installed)\n",
    "# !pip install bitnet-py\n",
    "\n",
    "# Import the new library\n",
    "import bitnet_py as bitnet\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"bitnet_py version: {bitnet.__version__}\")\n",
    "print(f\"System info: {bitnet.get_system_info()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Original vs New API Comparison\n",
    "\n",
    "Let's see how the API has changed (or rather, how it hasn't!):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original BitNet Python code pattern:\n",
    "original_code = '''\n",
    "import model as fast\n",
    "\n",
    "# Create model and generation arguments\n",
    "model_args = fast.ModelArgs(\n",
    "    dim=2560,\n",
    "    n_layers=30,\n",
    "    n_heads=20,\n",
    "    vocab_size=128256,\n",
    "    use_kernel=True\n",
    ")\n",
    "\n",
    "gen_args = fast.GenArgs(\n",
    "    gen_length=128,\n",
    "    temperature=0.8,\n",
    "    top_p=0.9,\n",
    "    use_sampling=True\n",
    ")\n",
    "\n",
    "# Build FastGen engine\n",
    "g = fast.FastGen.build(\n",
    "    ckpt_dir=\"path/to/checkpoint\",\n",
    "    gen_args=gen_args,\n",
    "    device=\"cuda:0\"\n",
    ")\n",
    "\n",
    "# Generate text\n",
    "prompts = [\"Hello, world!\"]\n",
    "tokens = [g.tokenizer.encode(p, bos=False, eos=False) for p in prompts]\n",
    "stats, results = g.generate_all(tokens, use_cuda_graphs=True)\n",
    "'''\n",
    "\n",
    "print(\"Original API:\")\n",
    "print(original_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New bitnet_py code (almost identical!):\n",
    "new_code = '''\n",
    "import bitnet_py as fast  # Only change needed!\n",
    "\n",
    "# Everything else remains the same\n",
    "model_args = fast.ModelArgs(\n",
    "    dim=2560,\n",
    "    n_layers=30,\n",
    "    n_heads=20,\n",
    "    vocab_size=128256,\n",
    "    use_kernel=True\n",
    ")\n",
    "\n",
    "gen_args = fast.GenArgs(\n",
    "    gen_length=128,\n",
    "    temperature=0.8,\n",
    "    top_p=0.9,\n",
    "    use_sampling=True\n",
    ")\n",
    "\n",
    "g = fast.FastGen.build(\n",
    "    ckpt_dir=\"path/to/checkpoint\",\n",
    "    gen_args=gen_args,\n",
    "    device=\"cuda:0\"\n",
    ")\n",
    "\n",
    "prompts = [\"Hello, world!\"]\n",
    "tokens = [g.tokenizer.encode(p, bos=False, eos=False) for p in prompts]\n",
    "stats, results = g.generate_all(tokens, use_cuda_graphs=True)\n",
    "'''\n",
    "\n",
    "print(\"New API (bitnet_py):\")\n",
    "print(new_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Practical Migration Example\n",
    "\n",
    "Let's demonstrate a practical migration with actual code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for our example\n",
    "# Note: Update these paths to point to your actual model files\n",
    "MODEL_PATH = \"path/to/your/model.gguf\"  # Update this!\n",
    "TOKENIZER_PATH = \"path/to/your/tokenizer.model\"  # Update this!\n",
    "\n",
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"Hello, my name is\",\n",
    "    \"The capital of France is\",\n",
    "    \"In the year 2024,\",\n",
    "    \"Artificial intelligence is\"\n",
    "]\n",
    "\n",
    "print(f\"Model path: {MODEL_PATH}\")\n",
    "print(f\"Tokenizer path: {TOKENIZER_PATH}\")\n",
    "print(f\"Test prompts: {len(test_prompts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if model files exist (for demo purposes)\n",
    "model_exists = Path(MODEL_PATH).exists()\n",
    "tokenizer_exists = Path(TOKENIZER_PATH).exists()\n",
    "\n",
    "print(f\"Model file exists: {model_exists}\")\n",
    "print(f\"Tokenizer file exists: {tokenizer_exists}\")\n",
    "\n",
    "if not (model_exists and tokenizer_exists):\n",
    "    print(\"\\n‚ö†Ô∏è  Note: Update MODEL_PATH and TOKENIZER_PATH to point to your actual files\")\n",
    "    print(\"For this demo, we'll show the API without actually loading models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Loading Models with bitnet_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Simple model loading (recommended for new code)\n",
    "def load_model_simple():\n",
    "    \"\"\"Simple model loading approach.\"\"\"\n",
    "    try:\n",
    "        print(\"Loading model with simple API...\")\n",
    "        model = bitnet.load_model(MODEL_PATH, device=\"cpu\")\n",
    "        tokenizer = bitnet.create_tokenizer(TOKENIZER_PATH)\n",
    "        \n",
    "        # Create inference engine\n",
    "        config = bitnet.InferenceConfig(\n",
    "            max_new_tokens=50,\n",
    "            temperature=0.8,\n",
    "            top_p=0.9,\n",
    "            do_sample=True\n",
    "        )\n",
    "        \n",
    "        engine = bitnet.SimpleInference(model, tokenizer, config)\n",
    "        return engine\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return None\n",
    "\n",
    "# Method 2: FastGen compatibility (for migrated code)\n",
    "def load_model_fastgen():\n",
    "    \"\"\"FastGen-compatible loading approach.\"\"\"\n",
    "    try:\n",
    "        print(\"Loading model with FastGen API...\")\n",
    "        \n",
    "        # Create arguments (same as original)\n",
    "        gen_args = bitnet.GenArgs(\n",
    "            gen_length=50,\n",
    "            temperature=0.8,\n",
    "            top_p=0.9,\n",
    "            use_sampling=True\n",
    "        )\n",
    "        \n",
    "        # Build FastGen engine (same API as original)\n",
    "        engine = bitnet.FastGen.build(\n",
    "            ckpt_dir=str(Path(MODEL_PATH).parent),\n",
    "            gen_args=gen_args,\n",
    "            device=\"cpu\"\n",
    "        )\n",
    "        \n",
    "        return engine\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return None\n",
    "\n",
    "# Try loading (will show API even if files don't exist)\n",
    "print(\"Demonstrating model loading APIs:\")\n",
    "print(\"\\n1. Simple API:\")\n",
    "simple_engine = load_model_simple()\n",
    "\n",
    "print(\"\\n2. FastGen API:\")\n",
    "fastgen_engine = load_model_fastgen()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Text Generation Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_generation(engine, engine_type=\"unknown\"):\n",
    "    \"\"\"Demonstrate text generation with the given engine.\"\"\"\n",
    "    if engine is None:\n",
    "        print(f\"‚ö†Ô∏è  {engine_type} engine not available (model files not found)\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nüöÄ Generating text with {engine_type} engine:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i, prompt in enumerate(test_prompts[:2]):  # Test first 2 prompts\n",
    "        try:\n",
    "            print(f\"\\nPrompt {i+1}: {prompt}\")\n",
    "            \n",
    "            start_time = time.time()\n",
    "            \n",
    "            if hasattr(engine, 'generate'):  # Simple API\n",
    "                response = engine.generate(prompt)\n",
    "            else:  # FastGen API\n",
    "                tokens = engine.tokenizer.encode(prompt, bos=False, eos=False)\n",
    "                stats, generated = engine.generate_all([tokens])\n",
    "                response = engine.tokenizer.decode(generated[0])\n",
    "            \n",
    "            generation_time = time.time() - start_time\n",
    "            \n",
    "            print(f\"Response: {response}\")\n",
    "            print(f\"Time: {generation_time:.3f}s\")\n",
    "            \n",
    "            results.append({\n",
    "                \"prompt\": prompt,\n",
    "                \"response\": response,\n",
    "                \"time\": generation_time\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error generating for '{prompt}': {e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Demonstrate generation with both engines\n",
    "simple_results = demonstrate_generation(simple_engine, \"Simple\")\n",
    "fastgen_results = demonstrate_generation(fastgen_engine, \"FastGen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_performance(results, engine_name):\n",
    "    \"\"\"Analyze performance results.\"\"\"\n",
    "    if not results:\n",
    "        print(f\"No results to analyze for {engine_name}\")\n",
    "        return\n",
    "    \n",
    "    times = [r['time'] for r in results]\n",
    "    total_chars = sum(len(r['response']) for r in results)\n",
    "    total_time = sum(times)\n",
    "    \n",
    "    print(f\"\\nüìä Performance Analysis - {engine_name}:\")\n",
    "    print(f\"  Total prompts: {len(results)}\")\n",
    "    print(f\"  Average time: {np.mean(times):.3f}s\")\n",
    "    print(f\"  Total characters: {total_chars}\")\n",
    "    print(f\"  Characters/second: {total_chars/total_time:.1f}\")\n",
    "    print(f\"  Min time: {min(times):.3f}s\")\n",
    "    print(f\"  Max time: {max(times):.3f}s\")\n",
    "\n",
    "# Analyze performance\n",
    "if simple_results:\n",
    "    analyze_performance(simple_results, \"Simple API\")\n",
    "\n",
    "if fastgen_results:\n",
    "    analyze_performance(fastgen_results, \"FastGen API\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Migration Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import migration utilities\n",
    "from bitnet_py.migration import MigrationHelper, migrate_project\n",
    "\n",
    "# Create migration helper\n",
    "helper = MigrationHelper(verbose=True)\n",
    "\n",
    "# Check if original BitNet installation is available\n",
    "print(\"Checking for original BitNet installation:\")\n",
    "original_available = helper.check_original_installation()\n",
    "print(f\"Original BitNet available: {original_available}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate code analysis\n",
    "sample_code = '''\n",
    "import model as fast\n",
    "import torch\n",
    "\n",
    "def main():\n",
    "    # Create model args\n",
    "    model_args = fast.ModelArgs(\n",
    "        dim=2560,\n",
    "        n_layers=30,\n",
    "        use_kernel=True\n",
    "    )\n",
    "    \n",
    "    # Build engine\n",
    "    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    g = fast.FastGen.build(\n",
    "        ckpt_dir=\"models/checkpoint\",\n",
    "        device=device\n",
    "    )\n",
    "    \n",
    "    # Generate text\n",
    "    prompt = \"Hello world\"\n",
    "    tokens = g.tokenizer.encode(prompt, bos=False, eos=False)\n",
    "    stats, results = g.generate_all([tokens], use_cuda_graphs=True)\n",
    "    \n",
    "    return g.tokenizer.decode(results[0])\n",
    "'''\n",
    "\n",
    "# Write sample code to temporary file\n",
    "import tempfile\n",
    "with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:\n",
    "    f.write(sample_code)\n",
    "    temp_file = f.name\n",
    "\n",
    "print(\"Analyzing sample code:\")\n",
    "analysis = helper.analyze_existing_code(temp_file)\n",
    "\n",
    "print(f\"\\nAnalysis Results:\")\n",
    "print(f\"  Compatible: {analysis['compatible']}\")\n",
    "print(f\"  Imports found: {len(analysis['imports'])}\")\n",
    "print(f\"  Issues: {len(analysis['issues'])}\")\n",
    "print(f\"  Suggestions: {len(analysis['suggestions'])}\")\n",
    "\n",
    "if analysis['suggestions']:\n",
    "    print(\"\\nSuggestions:\")\n",
    "    for suggestion in analysis['suggestions']:\n",
    "        print(f\"  - {suggestion}\")\n",
    "\n",
    "# Clean up\n",
    "import os\n",
    "os.unlink(temp_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Expected Performance Improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show expected performance improvements\n",
    "improvements = {\n",
    "    \"Inference Speed\": \"2-5x faster\",\n",
    "    \"Memory Usage\": \"50% reduction\",\n",
    "    \"Startup Time\": \"4x faster\",\n",
    "    \"CPU Utilization\": \"Better efficiency\",\n",
    "    \"Error Handling\": \"More robust\",\n",
    "    \"Dependencies\": \"Fewer required\"\n",
    "}\n",
    "\n",
    "print(\"Expected Performance Improvements:\")\n",
    "print(\"=\" * 40)\n",
    "for metric, improvement in improvements.items():\n",
    "    print(f\"{metric:20}: {improvement}\")\n",
    "\n",
    "# Show feature comparison\n",
    "features = {\n",
    "    \"Original BitNet\": [\n",
    "        \"Python implementation\",\n",
    "        \"PyTorch backend\",\n",
    "        \"xformers dependency\",\n",
    "        \"Manual CUDA management\",\n",
    "        \"Limited async support\"\n",
    "    ],\n",
    "    \"bitnet_py\": [\n",
    "        \"Rust implementation\",\n",
    "        \"Zero-cost abstractions\",\n",
    "        \"Built-in optimizations\",\n",
    "        \"Automatic device management\",\n",
    "        \"Full async/await support\",\n",
    "        \"Streaming generation\",\n",
    "        \"Better error handling\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"\\nFeature Comparison:\")\n",
    "print(\"=\" * 40)\n",
    "for impl, feature_list in features.items():\n",
    "    print(f\"\\n{impl}:\")\n",
    "    for feature in feature_list:\n",
    "        symbol = \"-\" if impl == \"Original BitNet\" else \"+\"\n",
    "        print(f\"  {symbol} {feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Migration Checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive migration checklist\n",
    "checklist = [\n",
    "    (\"Pre-Migration\", [\n",
    "        \"‚úÖ Backup your existing project\",\n",
    "        \"‚úÖ Document current performance baselines\",\n",
    "        \"‚úÖ Identify all BitNet Python dependencies\",\n",
    "        \"‚úÖ Test current implementation thoroughly\"\n",
    "    ]),\n",
    "    (\"Installation\", [\n",
    "        \"‚úÖ Install bitnet_py: pip install bitnet-py\",\n",
    "        \"‚úÖ Verify installation works\",\n",
    "        \"‚úÖ Check system compatibility\"\n",
    "    ]),\n",
    "    (\"Code Migration\", [\n",
    "        \"üîÑ Update imports: model ‚Üí bitnet_py\",\n",
    "        \"üîÑ Remove xformers dependencies\",\n",
    "        \"üîÑ Update device management code\",\n",
    "        \"üîÑ Test migrated code\"\n",
    "    ]),\n",
    "    (\"Validation\", [\n",
    "        \"‚è≥ Run side-by-side comparison\",\n",
    "        \"‚è≥ Validate output accuracy\",\n",
    "        \"‚è≥ Benchmark performance\",\n",
    "        \"‚è≥ Test error handling\"\n",
    "    ])\n",
    "]\n",
    "\n",
    "print(\"Migration Checklist:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for section, items in checklist:\n",
    "    print(f\"\\n{section}:\")\n",
    "    for item in items:\n",
    "        print(f\"  {item}\")\n",
    "\n",
    "print(\"\\nLegend:\")\n",
    "print(\"  ‚úÖ Completed in this notebook\")\n",
    "print(\"  üîÑ Ready to implement\")\n",
    "print(\"  ‚è≥ Next steps for your project\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated the basic migration process from BitNet Python to bitnet_py. Key takeaways:\n",
    "\n",
    "1. **Minimal Code Changes**: Most code requires only import statement changes\n",
    "2. **API Compatibility**: The FastGen API remains identical for easy migration\n",
    "3. **Performance Gains**: Expect 2-5x speed improvements and 50% memory reduction\n",
    "4. **Better Features**: Enhanced error handling, async support, and automatic optimizations\n",
    "5. **Migration Tools**: Automated utilities help analyze and migrate existing projects\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Update your model and tokenizer paths in this notebook\n",
    "2. Run the examples with your actual models\n",
    "3. Use the migration utilities on your existing projects\n",
    "4. Check out the other example notebooks for advanced features\n",
    "5. Read the comprehensive migration guide for detailed instructions\n",
    "\n",
    "## Resources\n",
    "\n",
    "- [Migration Guide](../MIGRATION_GUIDE.md)\n",
    "- [Performance Comparison Example](../performance_comparison.py)\n",
    "- [Migration Utilities](../python/bitnet_py/migration.py)\n",
    "- [API Documentation](https://docs.rs/bitnet-py/)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
