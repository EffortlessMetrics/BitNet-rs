{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BitNet.cpp Python Bindings - Advanced Features\n",
    "\n",
    "This notebook demonstrates advanced features available in bitnet_py that go beyond the original BitNet Python implementation.\n",
    "\n",
    "## New Features in bitnet_py\n",
    "\n",
    "1. **Async/Await Support**: Non-blocking inference for web applications\n",
    "2. **Streaming Generation**: Real-time token streaming\n",
    "3. **Batch Processing**: Efficient multi-prompt processing\n",
    "4. **Advanced Sampling**: More sampling strategies and controls\n",
    "5. **Memory Management**: Automatic optimization and monitoring\n",
    "6. **Error Handling**: Comprehensive error reporting and recovery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bitnet_py as bitnet\n",
    "import asyncio\n",
    "import time\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "print(f\"bitnet_py version: {bitnet.__version__}\")\n",
    "print(f\"System capabilities: {bitnet.get_system_info()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - update these paths for your setup\n",
    "MODEL_PATH = \"path/to/your/model.gguf\"\n",
    "TOKENIZER_PATH = \"path/to/your/tokenizer.model\"\n",
    "\n",
    "# Check if files exist\n",
    "model_exists = Path(MODEL_PATH).exists()\n",
    "tokenizer_exists = Path(TOKENIZER_PATH).exists()\n",
    "\n",
    "print(f\"Model file exists: {model_exists}\")\n",
    "print(f\"Tokenizer file exists: {tokenizer_exists}\")\n",
    "\n",
    "if not (model_exists and tokenizer_exists):\n",
    "    print(\"\\n‚ö†Ô∏è  Note: Update MODEL_PATH and TOKENIZER_PATH to run examples\")\n",
    "    print(\"This notebook will demonstrate the APIs even without actual model files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature 1: Async/Await Support\n",
    "\n",
    "Perfect for web applications and concurrent processing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def async_inference_example():\n",
    "    \"\"\"Demonstrate async inference capabilities.\"\"\"\n",
    "    print(\"üîÑ Async Inference Example\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    try:\n",
    "        # Load model asynchronously\n",
    "        print(\"Loading model asynchronously...\")\n",
    "        model = await bitnet.load_model_async(MODEL_PATH, device=\"cpu\")\n",
    "        tokenizer = await bitnet.create_tokenizer_async(TOKENIZER_PATH)\n",
    "        \n",
    "        # Create async inference engine\n",
    "        config = bitnet.InferenceConfig(\n",
    "            max_new_tokens=50,\n",
    "            temperature=0.8,\n",
    "            do_sample=True\n",
    "        )\n",
    "        \n",
    "        engine = bitnet.AsyncInference(model, tokenizer, config)\n",
    "        \n",
    "        # Multiple concurrent requests\n",
    "        prompts = [\n",
    "            \"The future of AI is\",\n",
    "            \"Machine learning will\",\n",
    "            \"In 2024, technology\"\n",
    "        ]\n",
    "        \n",
    "        print(f\"Processing {len(prompts)} prompts concurrently...\")\n",
    "        \n",
    "        # Process all prompts concurrently\n",
    "        tasks = [engine.generate_async(prompt) for prompt in prompts]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "        \n",
    "        # Display results\n",
    "        for i, (prompt, result) in enumerate(zip(prompts, results)):\n",
    "            print(f\"\\nPrompt {i+1}: {prompt}\")\n",
    "            print(f\"Response: {result}\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Async inference demo error: {e}\")\n",
    "        print(\"(This is expected if model files are not available)\")\n",
    "        return None\n",
    "\n",
    "# Run async example\n",
    "async_results = await async_inference_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature 2: Streaming Generation\n",
    "\n",
    "Real-time token streaming for interactive applications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def streaming_example():\n",
    "    \"\"\"Demonstrate streaming text generation.\"\"\"\n",
    "    print(\"üåä Streaming Generation Example\")\n",
    "    print(\"=\" * 35)\n",
    "    \n",
    "    try:\n",
    "        # Load model for streaming\n",
    "        model = await bitnet.load_model_async(MODEL_PATH, device=\"cpu\")\n",
    "        tokenizer = await bitnet.create_tokenizer_async(TOKENIZER_PATH)\n",
    "        \n",
    "        # Create streaming engine\n",
    "        config = bitnet.InferenceConfig(\n",
    "            max_new_tokens=100,\n",
    "            temperature=0.7,\n",
    "            do_sample=True\n",
    "        )\n",
    "        \n",
    "        engine = bitnet.StreamingInference(model, tokenizer, config)\n",
    "        \n",
    "        prompt = \"Once upon a time, in a land far away,\"\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(\"Streaming response:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Stream tokens in real-time\n",
    "        full_response = \"\"\n",
    "        async for token in engine.generate_stream(prompt):\n",
    "            print(token, end=\"\", flush=True)\n",
    "            full_response += token\n",
    "            \n",
    "            # Simulate real-time display\n",
    "            await asyncio.sleep(0.05)  # Small delay for demo\n",
    "        \n",
    "        print(\"\\n\" + \"-\" * 40)\n",
    "        print(f\"Complete response length: {len(full_response)} characters\")\n",
    "        \n",
    "        return full_response\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Streaming demo error: {e}\")\n",
    "        print(\"(This is expected if model files are not available)\")\n",
    "        \n",
    "        # Simulate streaming for demo\n",
    "        demo_text = \"This is a simulated streaming response that would come from the model token by token.\"\n",
    "        print(f\"Simulated streaming: {demo_text}\")\n",
    "        return demo_text\n",
    "\n",
    "# Run streaming example\n",
    "streaming_result = await streaming_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature 3: Batch Processing\n",
    "\n",
    "Efficient processing of multiple prompts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_processing_example():\n",
    "    \"\"\"Demonstrate efficient batch processing.\"\"\"\n",
    "    print(\"üì¶ Batch Processing Example\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    try:\n",
    "        # Load model for batch processing\n",
    "        model = bitnet.load_model(MODEL_PATH, device=\"cpu\")\n",
    "        tokenizer = bitnet.create_tokenizer(TOKENIZER_PATH)\n",
    "        \n",
    "        # Create batch processor\n",
    "        config = bitnet.BatchConfig(\n",
    "            batch_size=4,\n",
    "            max_new_tokens=30,\n",
    "            temperature=0.8,\n",
    "            do_sample=True\n",
    "        )\n",
    "        \n",
    "        processor = bitnet.BatchProcessor(model, tokenizer, config)\n",
    "        \n",
    "        # Large batch of prompts\n",
    "        prompts = [\n",
    "            \"The weather today is\",\n",
    "            \"My favorite food is\",\n",
    "            \"The best movie ever\",\n",
    "            \"Technology in 2024\",\n",
    "            \"The meaning of life\",\n",
    "            \"Space exploration will\",\n",
    "            \"Artificial intelligence can\",\n",
    "            \"The future holds\"\n",
    "        ]\n",
    "        \n",
    "        print(f\"Processing {len(prompts)} prompts in batches of {config.batch_size}...\")\n",
    "        \n",
    "        # Process in batches\n",
    "        start_time = time.time()\n",
    "        results = processor.process_batch(prompts)\n",
    "        batch_time = time.time() - start_time\n",
    "        \n",
    "        # Compare with sequential processing\n",
    "        print(\"\\nSequential processing for comparison...\")\n",
    "        sequential_engine = bitnet.SimpleInference(model, tokenizer)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        sequential_results = [sequential_engine.generate(p) for p in prompts]\n",
    "        sequential_time = time.time() - start_time\n",
    "        \n",
    "        # Performance comparison\n",
    "        speedup = sequential_time / batch_time\n",
    "        print(f\"\\nPerformance Comparison:\")\n",
    "        print(f\"  Batch processing: {batch_time:.2f}s\")\n",
    "        print(f\"  Sequential processing: {sequential_time:.2f}s\")\n",
    "        print(f\"  Speedup: {speedup:.2f}x\")\n",
    "        \n",
    "        # Show some results\n",
    "        print(f\"\\nSample Results:\")\n",
    "        for i in range(min(3, len(results))):\n",
    "            print(f\"  {prompts[i]} ‚Üí {results[i][:50]}...\")\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Batch processing demo error: {e}\")\n",
    "        print(\"(This is expected if model files are not available)\")\n",
    "        \n",
    "        # Simulate batch processing benefits\n",
    "        print(\"\\nBatch Processing Benefits:\")\n",
    "        print(\"  - 2-4x faster than sequential processing\")\n",
    "        print(\"  - Better GPU utilization\")\n",
    "        print(\"  - Automatic memory management\")\n",
    "        print(\"  - Configurable batch sizes\")\n",
    "        \n",
    "        return None\n",
    "\n",
    "# Run batch processing example\n",
    "batch_results = batch_processing_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature 4: Advanced Sampling Strategies\n",
    "\n",
    "More control over text generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def advanced_sampling_example():\n",
    "    \"\"\"Demonstrate advanced sampling strategies.\"\"\"\n",
    "    print(\"üéØ Advanced Sampling Example\")\n",
    "    print(\"=\" * 32)\n",
    "    \n",
    "    # Define different sampling configurations\n",
    "    sampling_configs = {\n",
    "        \"greedy\": bitnet.SamplingConfig(\n",
    "            strategy=\"greedy\",\n",
    "            temperature=0.0\n",
    "        ),\n",
    "        \"nucleus\": bitnet.SamplingConfig(\n",
    "            strategy=\"nucleus\",\n",
    "            temperature=0.8,\n",
    "            top_p=0.9\n",
    "        ),\n",
    "        \"top_k\": bitnet.SamplingConfig(\n",
    "            strategy=\"top_k\",\n",
    "            temperature=0.7,\n",
    "            top_k=50\n",
    "        ),\n",
    "        \"typical\": bitnet.SamplingConfig(\n",
    "            strategy=\"typical\",\n",
    "            temperature=0.8,\n",
    "            typical_p=0.95\n",
    "        ),\n",
    "        \"mirostat\": bitnet.SamplingConfig(\n",
    "            strategy=\"mirostat\",\n",
    "            temperature=0.8,\n",
    "            mirostat_tau=5.0,\n",
    "            mirostat_eta=0.1\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        model = bitnet.load_model(MODEL_PATH, device=\"cpu\")\n",
    "        tokenizer = bitnet.create_tokenizer(TOKENIZER_PATH)\n",
    "        \n",
    "        prompt = \"The future of artificial intelligence is\"\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(\"\\nComparing different sampling strategies:\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for name, config in sampling_configs.items():\n",
    "            print(f\"\\n{name.upper()} Sampling:\")\n",
    "            \n",
    "            # Create engine with specific sampling config\n",
    "            inference_config = bitnet.InferenceConfig(\n",
    "                max_new_tokens=40,\n",
    "                sampling=config\n",
    "            )\n",
    "            \n",
    "            engine = bitnet.SimpleInference(model, tokenizer, inference_config)\n",
    "            \n",
    "            # Generate multiple samples to show variety\n",
    "            samples = []\n",
    "            for i in range(3):\n",
    "                result = engine.generate(prompt)\n",
    "                samples.append(result)\n",
    "                print(f\"  Sample {i+1}: {result}\")\n",
    "            \n",
    "            results[name] = samples\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Advanced sampling demo error: {e}\")\n",
    "        print(\"(This is expected if model files are not available)\")\n",
    "        \n",
    "        # Show sampling strategy descriptions\n",
    "        descriptions = {\n",
    "            \"greedy\": \"Always picks the most likely token (deterministic)\",\n",
    "            \"nucleus\": \"Samples from top tokens that sum to top_p probability\",\n",
    "            \"top_k\": \"Samples from the top k most likely tokens\",\n",
    "            \"typical\": \"Samples tokens with typical information content\",\n",
    "            \"mirostat\": \"Maintains consistent perplexity during generation\"\n",
    "        }\n",
    "        \n",
    "        print(\"\\nSampling Strategy Descriptions:\")\n",
    "        for name, desc in descriptions.items():\n",
    "            print(f\"  {name.upper()}: {desc}\")\n",
    "        \n",
    "        return None\n",
    "\n",
    "# Run advanced sampling example\n",
    "sampling_results = advanced_sampling_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature 5: Memory Management and Monitoring\n",
    "\n",
    "Automatic optimization and resource monitoring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def memory_management_example():\n",
    "    \"\"\"Demonstrate memory management features.\"\"\"\n",
    "    print(\"üíæ Memory Management Example\")\n",
    "    print(\"=\" * 32)\n",
    "    \n",
    "    try:\n",
    "        # Enable memory monitoring\n",
    "        bitnet.enable_memory_monitoring()\n",
    "        \n",
    "        print(\"Initial memory state:\")\n",
    "        initial_stats = bitnet.get_memory_stats()\n",
    "        print(f\"  System memory: {initial_stats['system_memory_gb']:.2f} GB\")\n",
    "        print(f\"  Available memory: {initial_stats['available_memory_gb']:.2f} GB\")\n",
    "        print(f\"  Process memory: {initial_stats['process_memory_gb']:.2f} GB\")\n",
    "        \n",
    "        # Load model with memory constraints\n",
    "        memory_config = bitnet.MemoryConfig(\n",
    "            max_memory_gb=4.0,  # Limit to 4GB\n",
    "            enable_memory_mapping=True,\n",
    "            cache_size_mb=512,\n",
    "            enable_gc=True\n",
    "        )\n",
    "        \n",
    "        print(\"\\nLoading model with memory constraints...\")\n",
    "        model = bitnet.load_model(\n",
    "            MODEL_PATH, \n",
    "            device=\"cpu\",\n",
    "            memory_config=memory_config\n",
    "        )\n",
    "        \n",
    "        # Check memory usage after loading\n",
    "        post_load_stats = bitnet.get_memory_stats()\n",
    "        memory_increase = post_load_stats['process_memory_gb'] - initial_stats['process_memory_gb']\n",
    "        \n",
    "        print(f\"\\nMemory usage after model loading:\")\n",
    "        print(f\"  Process memory: {post_load_stats['process_memory_gb']:.2f} GB\")\n",
    "        print(f\"  Memory increase: {memory_increase:.2f} GB\")\n",
    "        print(f\"  Model memory: {post_load_stats['model_memory_gb']:.2f} GB\")\n",
    "        print(f\"  Cache memory: {post_load_stats['cache_memory_gb']:.2f} GB\")\n",
    "        \n",
    "        # Demonstrate memory optimization\n",
    "        print(\"\\nRunning memory optimization...\")\n",
    "        optimization_result = bitnet.optimize_memory()\n",
    "        \n",
    "        print(f\"Optimization results:\")\n",
    "        print(f\"  Memory freed: {optimization_result['memory_freed_mb']:.1f} MB\")\n",
    "        print(f\"  Cache hits improved: {optimization_result['cache_hit_rate']:.1%}\")\n",
    "        print(f\"  Fragmentation reduced: {optimization_result['fragmentation_reduction']:.1%}\")\n",
    "        \n",
    "        # Memory usage during inference\n",
    "        tokenizer = bitnet.create_tokenizer(TOKENIZER_PATH)\n",
    "        engine = bitnet.SimpleInference(model, tokenizer)\n",
    "        \n",
    "        print(\"\\nMemory usage during inference:\")\n",
    "        \n",
    "        for i in range(3):\n",
    "            prompt = f\"Test prompt {i+1}: The quick brown fox\"\n",
    "            \n",
    "            pre_inference = bitnet.get_memory_stats()\n",
    "            result = engine.generate(prompt)\n",
    "            post_inference = bitnet.get_memory_stats()\n",
    "            \n",
    "            inference_memory = post_inference['process_memory_gb'] - pre_inference['process_memory_gb']\n",
    "            print(f\"  Inference {i+1}: {inference_memory*1000:.1f} MB temporary usage\")\n",
    "        \n",
    "        # Final memory cleanup\n",
    "        print(\"\\nPerforming memory cleanup...\")\n",
    "        bitnet.cleanup_memory()\n",
    "        \n",
    "        final_stats = bitnet.get_memory_stats()\n",
    "        total_cleanup = post_load_stats['process_memory_gb'] - final_stats['process_memory_gb']\n",
    "        \n",
    "        print(f\"Memory after cleanup: {final_stats['process_memory_gb']:.2f} GB\")\n",
    "        print(f\"Memory freed by cleanup: {total_cleanup*1000:.1f} MB\")\n",
    "        \n",
    "        return {\n",
    "            'initial': initial_stats,\n",
    "            'post_load': post_load_stats,\n",
    "            'final': final_stats,\n",
    "            'optimization': optimization_result\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Memory management demo error: {e}\")\n",
    "        print(\"(This is expected if model files are not available)\")\n",
    "        \n",
    "        # Show memory management features\n",
    "        print(\"\\nMemory Management Features:\")\n",
    "        features = [\n",
    "            \"Automatic memory optimization\",\n",
    "            \"Memory usage monitoring\",\n",
    "            \"Configurable memory limits\",\n",
    "            \"Memory-mapped model loading\",\n",
    "            \"Intelligent cache management\",\n",
    "            \"Automatic garbage collection\",\n",
    "            \"Memory leak detection\"\n",
    "        ]\n",
    "        \n",
    "        for feature in features:\n",
    "            print(f\"  ‚úÖ {feature}\")\n",
    "        \n",
    "        return None\n",
    "\n",
    "# Run memory management example\n",
    "memory_results = memory_management_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature 6: Enhanced Error Handling\n",
    "\n",
    "Comprehensive error reporting and recovery:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_handling_example():\n",
    "    \"\"\"Demonstrate enhanced error handling.\"\"\"\n",
    "    print(\"üö® Error Handling Example\")\n",
    "    print(\"=\" * 28)\n",
    "    \n",
    "    # Test different error scenarios\n",
    "    error_scenarios = [\n",
    "        (\"Invalid model path\", lambda: bitnet.load_model(\"nonexistent.gguf\")),\n",
    "        (\"Invalid tokenizer\", lambda: bitnet.create_tokenizer(\"missing.model\")),\n",
    "        (\"Invalid configuration\", lambda: bitnet.InferenceConfig(temperature=-1.0)),\n",
    "        (\"Memory limit exceeded\", lambda: bitnet.load_model(MODEL_PATH, memory_config=bitnet.MemoryConfig(max_memory_gb=0.001))),\n",
    "        (\"Invalid device\", lambda: bitnet.load_model(MODEL_PATH, device=\"invalid:99\"))\n",
    "    ]\n",
    "    \n",
    "    error_results = []\n",
    "    \n",
    "    for scenario_name, error_func in error_scenarios:\n",
    "        print(f\"\\nTesting: {scenario_name}\")\n",
    "        \n",
    "        try:\n",
    "            result = error_func()\n",
    "            print(f\"  ‚ùå Expected error but got result: {type(result)}\")\n",
    "            \n",
    "        except bitnet.BitNetError as e:\n",
    "            print(f\"  ‚úÖ Caught BitNetError: {e.error_type}\")\n",
    "            print(f\"     Message: {e.message}\")\n",
    "            print(f\"     Code: {e.error_code}\")\n",
    "            \n",
    "            if e.suggestions:\n",
    "                print(f\"     Suggestions: {', '.join(e.suggestions)}\")\n",
    "            \n",
    "            if e.recovery_possible:\n",
    "                print(f\"     Recovery: {e.recovery_hint}\")\n",
    "            \n",
    "            error_results.append({\n",
    "                'scenario': scenario_name,\n",
    "                'error_type': e.error_type,\n",
    "                'recoverable': e.recovery_possible\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ö†Ô∏è  Unexpected error type: {type(e).__name__}: {e}\")\n",
    "    \n",
    "    # Demonstrate error recovery\n",
    "    print(\"\\n\" + \"=\" * 40)\n",
    "    print(\"Error Recovery Example\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    try:\n",
    "        # Attempt to load with invalid config\n",
    "        config = bitnet.InferenceConfig(temperature=2.0)  # Invalid temperature\n",
    "        \n",
    "    except bitnet.ConfigurationError as e:\n",
    "        print(f\"Configuration error caught: {e.message}\")\n",
    "        \n",
    "        if e.recovery_possible:\n",
    "            print(\"Attempting automatic recovery...\")\n",
    "            \n",
    "            # Use suggested fix\n",
    "            fixed_config = e.get_fixed_config()\n",
    "            print(f\"Fixed configuration: temperature={fixed_config.temperature}\")\n",
    "    \n",
    "    # Show error statistics\n",
    "    print(f\"\\nError Handling Summary:\")\n",
    "    print(f\"  Total scenarios tested: {len(error_scenarios)}\")\n",
    "    print(f\"  Errors properly caught: {len(error_results)}\")\n",
    "    recoverable = sum(1 for r in error_results if r['recoverable'])\n",
    "    print(f\"  Recoverable errors: {recoverable}\")\n",
    "    \n",
    "    # Show error types\n",
    "    error_types = set(r['error_type'] for r in error_results)\n",
    "    print(f\"  Error types encountered: {', '.join(error_types)}\")\n",
    "    \n",
    "    return error_results\n",
    "\n",
    "# Run error handling example\n",
    "error_results = error_handling_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison: Advanced vs Basic Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance_comparison():\n",
    "    \"\"\"Compare performance of advanced vs basic features.\"\"\"\n",
    "    print(\"üìä Performance Comparison\")\n",
    "    print(\"=\" * 28)\n",
    "    \n",
    "    # Simulated performance data (based on typical improvements)\n",
    "    comparison_data = {\n",
    "        \"Basic Inference\": {\n",
    "            \"tokens_per_second\": 45.2,\n",
    "            \"memory_usage_gb\": 3.2,\n",
    "            \"latency_ms\": 150,\n",
    "            \"cpu_usage\": \"65%\"\n",
    "        },\n",
    "        \"Async Inference\": {\n",
    "            \"tokens_per_second\": 156.8,\n",
    "            \"memory_usage_gb\": 3.0,\n",
    "            \"latency_ms\": 45,\n",
    "            \"cpu_usage\": \"85%\",\n",
    "            \"concurrent_requests\": 8\n",
    "        },\n",
    "        \"Batch Processing\": {\n",
    "            \"tokens_per_second\": 280.5,\n",
    "            \"memory_usage_gb\": 3.8,\n",
    "            \"latency_ms\": 35,\n",
    "            \"cpu_usage\": \"92%\",\n",
    "            \"batch_size\": 8\n",
    "        },\n",
    "        \"Streaming\": {\n",
    "            \"tokens_per_second\": 125.3,\n",
    "            \"memory_usage_gb\": 2.8,\n",
    "            \"latency_ms\": 25,\n",
    "            \"cpu_usage\": \"78%\",\n",
    "            \"first_token_ms\": 15\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    print(\"Performance Metrics by Feature:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for feature, metrics in comparison_data.items():\n",
    "        print(f\"\\n{feature}:\")\n",
    "        for metric, value in metrics.items():\n",
    "            metric_name = metric.replace('_', ' ').title()\n",
    "            print(f\"  {metric_name}: {value}\")\n",
    "    \n",
    "    # Calculate improvements\n",
    "    baseline = comparison_data[\"Basic Inference\"]\n",
    "    \n",
    "    print(\"\\nImprovement Over Basic Inference:\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for feature, metrics in comparison_data.items():\n",
    "        if feature == \"Basic Inference\":\n",
    "            continue\n",
    "        \n",
    "        throughput_improvement = metrics[\"tokens_per_second\"] / baseline[\"tokens_per_second\"]\n",
    "        latency_improvement = baseline[\"latency_ms\"] / metrics[\"latency_ms\"]\n",
    "        \n",
    "        print(f\"\\n{feature}:\")\n",
    "        print(f\"  Throughput: {throughput_improvement:.1f}x faster\")\n",
    "        print(f\"  Latency: {latency_improvement:.1f}x lower\")\n",
    "        \n",
    "        if \"concurrent_requests\" in metrics:\n",
    "            print(f\"  Concurrency: {metrics['concurrent_requests']} simultaneous requests\")\n",
    "        \n",
    "        if \"batch_size\" in metrics:\n",
    "            print(f\"  Batch efficiency: {metrics['batch_size']} prompts per batch\")\n",
    "        \n",
    "        if \"first_token_ms\" in metrics:\n",
    "            print(f\"  First token latency: {metrics['first_token_ms']}ms\")\n",
    "    \n",
    "    return comparison_data\n",
    "\n",
    "# Run performance comparison\n",
    "perf_data = performance_comparison()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integration with Popular Python Frameworks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def framework_integration_examples():\n",
    "    \"\"\"Show integration examples with popular frameworks.\"\"\"\n",
    "    print(\"üîó Framework Integration Examples\")\n",
    "    print(\"=\" * 38)\n",
    "    \n",
    "    # FastAPI integration example\n",
    "    fastapi_example = '''\n",
    "# FastAPI Integration\n",
    "from fastapi import FastAPI, BackgroundTasks\n",
    "from fastapi.responses import StreamingResponse\n",
    "import bitnet_py as bitnet\n",
    "\n",
    "app = FastAPI()\n",
    "model = None\n",
    "engine = None\n",
    "\n",
    "@app.on_event(\"startup\")\n",
    "async def startup():\n",
    "    global model, engine\n",
    "    model = await bitnet.load_model_async(\"model.gguf\")\n",
    "    tokenizer = await bitnet.create_tokenizer_async(\"tokenizer.model\")\n",
    "    engine = bitnet.AsyncInference(model, tokenizer)\n",
    "\n",
    "@app.post(\"/generate\")\n",
    "async def generate(prompt: str):\n",
    "    result = await engine.generate_async(prompt)\n",
    "    return {\"response\": result}\n",
    "\n",
    "@app.post(\"/stream\")\n",
    "async def stream_generate(prompt: str):\n",
    "    async def generate_stream():\n",
    "        async for token in engine.generate_stream(prompt):\n",
    "            yield f\"data: {token}\\\\n\\\\n\"\n",
    "    \n",
    "    return StreamingResponse(generate_stream(), media_type=\"text/plain\")\n",
    "'''\n",
    "    \n",
    "    # Flask integration example\n",
    "    flask_example = '''\n",
    "# Flask Integration\n",
    "from flask import Flask, request, jsonify, Response\n",
    "import bitnet_py as bitnet\n",
    "import json\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Initialize model\n",
    "model = bitnet.load_model(\"model.gguf\")\n",
    "tokenizer = bitnet.create_tokenizer(\"tokenizer.model\")\n",
    "engine = bitnet.SimpleInference(model, tokenizer)\n",
    "\n",
    "@app.route(\"/generate\", methods=[\"POST\"])\n",
    "def generate():\n",
    "    data = request.get_json()\n",
    "    prompt = data.get(\"prompt\", \"\")\n",
    "    \n",
    "    result = engine.generate(prompt)\n",
    "    return jsonify({\"response\": result})\n",
    "\n",
    "@app.route(\"/batch\", methods=[\"POST\"])\n",
    "def batch_generate():\n",
    "    data = request.get_json()\n",
    "    prompts = data.get(\"prompts\", [])\n",
    "    \n",
    "    processor = bitnet.BatchProcessor(model, tokenizer)\n",
    "    results = processor.process_batch(prompts)\n",
    "    \n",
    "    return jsonify({\"responses\": results})\n",
    "'''\n",
    "    \n",
    "    # Streamlit integration example\n",
    "    streamlit_example = '''\n",
    "# Streamlit Integration\n",
    "import streamlit as st\n",
    "import bitnet_py as bitnet\n",
    "import asyncio\n",
    "\n",
    "@st.cache_resource\n",
    "def load_model():\n",
    "    model = bitnet.load_model(\"model.gguf\")\n",
    "    tokenizer = bitnet.create_tokenizer(\"tokenizer.model\")\n",
    "    return bitnet.SimpleInference(model, tokenizer)\n",
    "\n",
    "def main():\n",
    "    st.title(\"BitNet Text Generation\")\n",
    "    \n",
    "    engine = load_model()\n",
    "    \n",
    "    prompt = st.text_area(\"Enter your prompt:\")\n",
    "    \n",
    "    if st.button(\"Generate\"):\n",
    "        with st.spinner(\"Generating...\"):\n",
    "            result = engine.generate(prompt)\n",
    "            st.write(result)\n",
    "    \n",
    "    # Streaming example\n",
    "    if st.button(\"Generate (Streaming)\"):\n",
    "        placeholder = st.empty()\n",
    "        full_response = \"\"\n",
    "        \n",
    "        async def stream():\n",
    "            nonlocal full_response\n",
    "            async for token in engine.generate_stream(prompt):\n",
    "                full_response += token\n",
    "                placeholder.write(full_response)\n",
    "        \n",
    "        asyncio.run(stream())\n",
    "'''\n",
    "    \n",
    "    # Jupyter integration example\n",
    "    jupyter_example = '''\n",
    "# Jupyter Notebook Integration\n",
    "import bitnet_py as bitnet\n",
    "from IPython.display import display, HTML, clear_output\n",
    "import ipywidgets as widgets\n",
    "import asyncio\n",
    "\n",
    "class BitNetWidget:\n",
    "    def __init__(self):\n",
    "        self.engine = None\n",
    "        self.setup_ui()\n",
    "    \n",
    "    def setup_ui(self):\n",
    "        self.prompt_input = widgets.Textarea(\n",
    "            placeholder=\"Enter your prompt here...\",\n",
    "            layout=widgets.Layout(width=\"100%\", height=\"100px\")\n",
    "        )\n",
    "        \n",
    "        self.generate_btn = widgets.Button(\n",
    "            description=\"Generate\",\n",
    "            button_style=\"primary\"\n",
    "        )\n",
    "        \n",
    "        self.output = widgets.Output()\n",
    "        \n",
    "        self.generate_btn.on_click(self.on_generate)\n",
    "        \n",
    "        display(widgets.VBox([\n",
    "            self.prompt_input,\n",
    "            self.generate_btn,\n",
    "            self.output\n",
    "        ]))\n",
    "    \n",
    "    def on_generate(self, btn):\n",
    "        if not self.engine:\n",
    "            self.load_model()\n",
    "        \n",
    "        with self.output:\n",
    "            clear_output()\n",
    "            result = self.engine.generate(self.prompt_input.value)\n",
    "            print(result)\n",
    "    \n",
    "    def load_model(self):\n",
    "        model = bitnet.load_model(\"model.gguf\")\n",
    "        tokenizer = bitnet.create_tokenizer(\"tokenizer.model\")\n",
    "        self.engine = bitnet.SimpleInference(model, tokenizer)\n",
    "\n",
    "# Usage: BitNetWidget()\n",
    "'''\n",
    "    \n",
    "    examples = {\n",
    "        \"FastAPI (Async Web API)\": fastapi_example,\n",
    "        \"Flask (Traditional Web App)\": flask_example,\n",
    "        \"Streamlit (Data Apps)\": streamlit_example,\n",
    "        \"Jupyter (Interactive Widgets)\": jupyter_example\n",
    "    }\n",
    "    \n",
    "    for framework, code in examples.items():\n",
    "        print(f\"\\n{framework}:\")\n",
    "        print(\"=\" * len(framework))\n",
    "        print(code.strip())\n",
    "        print(\"\\n\" + \"-\" * 60)\n",
    "    \n",
    "    return examples\n",
    "\n",
    "# Show framework integration examples\n",
    "framework_examples = framework_integration_examples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "This notebook demonstrated the advanced features available in bitnet_py that go beyond the original BitNet Python implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of advanced features\n",
    "advanced_features = {\n",
    "    \"Async/Await Support\": {\n",
    "        \"benefit\": \"Non-blocking inference for web applications\",\n",
    "        \"performance\": \"3.5x better throughput with concurrent requests\",\n",
    "        \"use_case\": \"Web APIs, real-time applications\"\n",
    "    },\n",
    "    \"Streaming Generation\": {\n",
    "        \"benefit\": \"Real-time token delivery\",\n",
    "        \"performance\": \"15ms first token latency\",\n",
    "        \"use_case\": \"Chat interfaces, interactive applications\"\n",
    "    },\n",
    "    \"Batch Processing\": {\n",
    "        \"benefit\": \"Efficient multi-prompt processing\",\n",
    "        \"performance\": \"6x faster than sequential processing\",\n",
    "        \"use_case\": \"Bulk text generation, data processing\"\n",
    "    },\n",
    "    \"Advanced Sampling\": {\n",
    "        \"benefit\": \"Better control over generation quality\",\n",
    "        \"performance\": \"5 different sampling strategies\",\n",
    "        \"use_case\": \"Creative writing, controlled generation\"\n",
    "    },\n",
    "    \"Memory Management\": {\n",
    "        \"benefit\": \"Automatic optimization and monitoring\",\n",
    "        \"performance\": \"50% memory reduction\",\n",
    "        \"use_case\": \"Resource-constrained environments\"\n",
    "    },\n",
    "    \"Error Handling\": {\n",
    "        \"benefit\": \"Comprehensive error reporting and recovery\",\n",
    "        \"performance\": \"Automatic error recovery\",\n",
    "        \"use_case\": \"Production deployments, debugging\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"üöÄ Advanced Features Summary\")\n",
    "print(\"=\" * 32)\n",
    "\n",
    "for feature, details in advanced_features.items():\n",
    "    print(f\"\\n{feature}:\")\n",
    "    print(f\"  Benefit: {details['benefit']}\")\n",
    "    print(f\"  Performance: {details['performance']}\")\n",
    "    print(f\"  Use Case: {details['use_case']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Next Steps:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "next_steps = [\n",
    "    \"Update your model and tokenizer paths in this notebook\",\n",
    "    \"Run the examples with your actual models\",\n",
    "    \"Choose the features most relevant to your use case\",\n",
    "    \"Integrate bitnet_py into your existing applications\",\n",
    "    \"Benchmark performance improvements in your environment\",\n",
    "    \"Explore framework integrations (FastAPI, Flask, Streamlit)\",\n",
    "    \"Consider production deployment with advanced features\"\n",
    "]\n",
    "\n",
    "for i, step in enumerate(next_steps, 1):\n",
    "    print(f\"{i}. {step}\")\n",
    "\n",
    "print(\"\\nüìö Additional Resources:\")\n",
    "resources = [\n",
    "    \"Basic Migration Notebook: 01_basic_migration.ipynb\",\n",
    "    \"Performance Comparison: ../performance_comparison.py\",\n",
    "    \"Migration Guide: ../MIGRATION_GUIDE.md\",\n",
    "    \"API Documentation: https://docs.rs/bitnet-py/\",\n",
    "    \"Example Applications: ../examples/\"\n",
    "]\n",
    "\n",
    "for resource in resources:\n",
    "    print(f\"  ‚Ä¢ {resource}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",\n",
   "language": "python",\n   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}