name: Testing Framework - Master Workflow

on:
  push:
    branches: [main, develop]
    paths:
      - "tests/**"
      - "crates/**"
      - "Cargo.toml"
      - "Cargo.lock"
      - ".github/workflows/testing-framework-*.yml"
  pull_request:
    branches: [main, develop]
    paths:
      - "tests/**"
      - "crates/**"
      - "Cargo.toml"
      - "Cargo.lock"
      - ".github/workflows/testing-framework-*.yml"
  workflow_dispatch:
    inputs:
      run_crossval:
        description: "Run cross-validation tests"
        required: false
        default: false
        type: boolean
      run_performance:
        description: "Run performance benchmarks"
        required: false
        default: false
        type: boolean
      coverage_threshold:
        description: "Coverage threshold (%)"
        required: false
        default: "90"
        type: string

env:
  CARGO_TERM_COLOR: always
  RUST_BACKTRACE: 1

jobs:
  # Determine which workflows to run based on changes and inputs
  workflow-planning:
    name: Plan Testing Framework Execution
    runs-on: ubuntu-latest
    outputs:
      run-unit-tests: ${{ steps.plan.outputs.run-unit-tests }}
      run-integration-tests: ${{ steps.plan.outputs.run-integration-tests }}
      run-coverage: ${{ steps.plan.outputs.run-coverage }}
      run-crossval: ${{ steps.plan.outputs.run-crossval }}
      run-performance: ${{ steps.plan.outputs.run-performance }}
      run-cache-optimization: ${{ steps.plan.outputs.run-cache-optimization }}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Plan workflow execution
        id: plan
        run: |
          echo "Planning testing framework execution..."

          # Always run core tests
          echo "run-unit-tests=true" >> $GITHUB_OUTPUT
          echo "run-integration-tests=true" >> $GITHUB_OUTPUT
          echo "run-coverage=true" >> $GITHUB_OUTPUT
          echo "run-cache-optimization=true" >> $GITHUB_OUTPUT

          # Cross-validation: manual trigger, labeled PRs, or main branch pushes
          if [[ "${{ github.event.inputs.run_crossval }}" == "true" ]] || \
             [[ "${{ contains(github.event.pull_request.labels.*.name, 'crossval') }}" == "true" ]] || \
             [[ "${{ github.ref }}" == "refs/heads/main" ]]; then
            echo "run-crossval=true" >> $GITHUB_OUTPUT
          else
            echo "run-crossval=false" >> $GITHUB_OUTPUT
          fi

          # Performance: manual trigger, performance-related changes, or main branch
          if [[ "${{ github.event.inputs.run_performance }}" == "true" ]] || \
             [[ "${{ github.ref }}" == "refs/heads/main" ]]; then
            echo "run-performance=true" >> $GITHUB_OUTPUT
          else
            # Check for performance-related changes
            if git diff --name-only HEAD~1 HEAD | grep -E "(benches/|performance|benchmark)" > /dev/null; then
              echo "run-performance=true" >> $GITHUB_OUTPUT
            else
              echo "run-performance=false" >> $GITHUB_OUTPUT
            fi
          fi

          echo "Workflow execution plan:"
          echo "- Unit Tests: $(cat $GITHUB_OUTPUT | grep run-unit-tests | cut -d= -f2)"
          echo "- Integration Tests: $(cat $GITHUB_OUTPUT | grep run-integration-tests | cut -d= -f2)"
          echo "- Coverage: $(cat $GITHUB_OUTPUT | grep run-coverage | cut -d= -f2)"
          echo "- Cross-Validation: $(cat $GITHUB_OUTPUT | grep run-crossval | cut -d= -f2)"
          echo "- Performance: $(cat $GITHUB_OUTPUT | grep run-performance | cut -d= -f2)"
          echo "- Cache Optimization: $(cat $GITHUB_OUTPUT | grep run-cache-optimization | cut -d= -f2)"

  # Core testing workflows (always run)
  unit-tests:
    name: Unit Tests
    needs: workflow-planning
    if: needs.workflow-planning.outputs.run-unit-tests == 'true'
    uses: ./.github/workflows/testing-framework-unit.yml
    with:
      coverage_threshold: ${{ github.event.inputs.coverage_threshold || '90' }}

  integration-tests:
    name: Integration Tests
    needs: workflow-planning
    if: needs.workflow-planning.outputs.run-integration-tests == 'true'
    uses: ./.github/workflows/testing-framework-integration.yml
    with:
      test_timeout: "30"

  coverage-collection:
    name: Coverage Collection
    needs: workflow-planning
    if: needs.workflow-planning.outputs.run-coverage == 'true'
    uses: ./.github/workflows/testing-framework-coverage.yml
    with:
      coverage_threshold: ${{ github.event.inputs.coverage_threshold || '90' }}
      include_integration: true

  cache-optimization:
    name: Cache & Optimization
    needs: workflow-planning
    if: needs.workflow-planning.outputs.run-cache-optimization == 'true'
    uses: ./.github/workflows/testing-framework-cache-optimization.yml
    with:
      force_all_tests: false
      cache_strategy: "smart"

  # Optional workflows (conditional execution)
  cross-validation:
    name: Cross-Validation
    needs: workflow-planning
    if: needs.workflow-planning.outputs.run-crossval == 'true'
    uses: ./.github/workflows/testing-framework-crossval.yml
    with:
      tolerance: "1e-6"
      force_cpp_rebuild: false

  performance-benchmarks:
    name: Performance Benchmarks
    needs: workflow-planning
    if: needs.workflow-planning.outputs.run-performance == 'true'
    uses: ./.github/workflows/testing-framework-performance.yml
    with:
      benchmark_duration: "60"
      regression_threshold: "5"

  # Comprehensive reporting and status
  testing-framework-summary:
    name: Testing Framework Summary
    needs:
      [
        workflow-planning,
        unit-tests,
        integration-tests,
        coverage-collection,
        cache-optimization,
        cross-validation,
        performance-benchmarks,
      ]
    if: always()
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download all workflow artifacts
        uses: actions/download-artifact@v4
        with:
          path: workflow-artifacts
        continue-on-error: true

      - name: Generate comprehensive summary
        run: |
          echo "Generating comprehensive testing framework summary..."

          cat > testing-framework-summary.md << 'EOF'
          # üß™ Testing Framework Execution Summary

          **Date**: $(date -u +"%Y-%m-%d %H:%M:%S UTC")
          **Trigger**: ${{ github.event_name }}
          **Branch**: ${{ github.ref_name }}
          **Commit**: ${{ github.sha }}

          ## Workflow Execution Plan

          EOF

          # Add execution plan
          echo "- **Unit Tests**: ${{ needs.workflow-planning.outputs.run-unit-tests }}" >> testing-framework-summary.md
          echo "- **Integration Tests**: ${{ needs.workflow-planning.outputs.run-integration-tests }}" >> testing-framework-summary.md
          echo "- **Coverage Collection**: ${{ needs.workflow-planning.outputs.run-coverage }}" >> testing-framework-summary.md
          echo "- **Cache Optimization**: ${{ needs.workflow-planning.outputs.run-cache-optimization }}" >> testing-framework-summary.md
          echo "- **Cross-Validation**: ${{ needs.workflow-planning.outputs.run-crossval }}" >> testing-framework-summary.md
          echo "- **Performance Benchmarks**: ${{ needs.workflow-planning.outputs.run-performance }}" >> testing-framework-summary.md

          echo "" >> testing-framework-summary.md
          echo "## Workflow Results" >> testing-framework-summary.md
          echo "" >> testing-framework-summary.md

          # Check results for each workflow
          if [[ "${{ needs.unit-tests.result }}" == "success" ]]; then
            echo "- ‚úÖ **Unit Tests**: All tests passed" >> testing-framework-summary.md
          elif [[ "${{ needs.unit-tests.result }}" == "failure" ]]; then
            echo "- ‚ùå **Unit Tests**: Tests failed" >> testing-framework-summary.md
          elif [[ "${{ needs.unit-tests.result }}" == "skipped" ]]; then
            echo "- ‚è≠Ô∏è **Unit Tests**: Skipped" >> testing-framework-summary.md
          else
            echo "- ‚ö†Ô∏è **Unit Tests**: ${{ needs.unit-tests.result }}" >> testing-framework-summary.md
          fi

          if [[ "${{ needs.integration-tests.result }}" == "success" ]]; then
            echo "- ‚úÖ **Integration Tests**: All tests passed" >> testing-framework-summary.md
          elif [[ "${{ needs.integration-tests.result }}" == "failure" ]]; then
            echo "- ‚ùå **Integration Tests**: Tests failed" >> testing-framework-summary.md
          elif [[ "${{ needs.integration-tests.result }}" == "skipped" ]]; then
            echo "- ‚è≠Ô∏è **Integration Tests**: Skipped" >> testing-framework-summary.md
          else
            echo "- ‚ö†Ô∏è **Integration Tests**: ${{ needs.integration-tests.result }}" >> testing-framework-summary.md
          fi

          if [[ "${{ needs.coverage-collection.result }}" == "success" ]]; then
            echo "- ‚úÖ **Coverage Collection**: Coverage targets met" >> testing-framework-summary.md
          elif [[ "${{ needs.coverage-collection.result }}" == "failure" ]]; then
            echo "- ‚ùå **Coverage Collection**: Coverage below threshold" >> testing-framework-summary.md
          elif [[ "${{ needs.coverage-collection.result }}" == "skipped" ]]; then
            echo "- ‚è≠Ô∏è **Coverage Collection**: Skipped" >> testing-framework-summary.md
          else
            echo "- ‚ö†Ô∏è **Coverage Collection**: ${{ needs.coverage-collection.result }}" >> testing-framework-summary.md
          fi

          if [[ "${{ needs.cache-optimization.result }}" == "success" ]]; then
            echo "- ‚úÖ **Cache Optimization**: Optimization successful" >> testing-framework-summary.md
          elif [[ "${{ needs.cache-optimization.result }}" == "failure" ]]; then
            echo "- ‚ùå **Cache Optimization**: Optimization failed" >> testing-framework-summary.md
          elif [[ "${{ needs.cache-optimization.result }}" == "skipped" ]]; then
            echo "- ‚è≠Ô∏è **Cache Optimization**: Skipped" >> testing-framework-summary.md
          else
            echo "- ‚ö†Ô∏è **Cache Optimization**: ${{ needs.cache-optimization.result }}" >> testing-framework-summary.md
          fi

          if [[ "${{ needs.cross-validation.result }}" == "success" ]]; then
            echo "- ‚úÖ **Cross-Validation**: Rust/C++ parity verified" >> testing-framework-summary.md
          elif [[ "${{ needs.cross-validation.result }}" == "failure" ]]; then
            echo "- ‚ùå **Cross-Validation**: Parity issues detected" >> testing-framework-summary.md
          elif [[ "${{ needs.cross-validation.result }}" == "skipped" ]]; then
            echo "- ‚è≠Ô∏è **Cross-Validation**: Skipped" >> testing-framework-summary.md
          else
            echo "- ‚ö†Ô∏è **Cross-Validation**: ${{ needs.cross-validation.result }}" >> testing-framework-summary.md
          fi

          if [[ "${{ needs.performance-benchmarks.result }}" == "success" ]]; then
            echo "- ‚úÖ **Performance Benchmarks**: No regressions detected" >> testing-framework-summary.md
          elif [[ "${{ needs.performance-benchmarks.result }}" == "failure" ]]; then
            echo "- ‚ùå **Performance Benchmarks**: Performance issues detected" >> testing-framework-summary.md
          elif [[ "${{ needs.performance-benchmarks.result }}" == "skipped" ]]; then
            echo "- ‚è≠Ô∏è **Performance Benchmarks**: Skipped" >> testing-framework-summary.md
          else
            echo "- ‚ö†Ô∏è **Performance Benchmarks**: ${{ needs.performance-benchmarks.result }}" >> testing-framework-summary.md
          fi

          echo "" >> testing-framework-summary.md
          echo "## Quality Metrics" >> testing-framework-summary.md
          echo "" >> testing-framework-summary.md
          echo "- üéØ **Coverage Threshold**: ${{ github.event.inputs.coverage_threshold || '90' }}%" >> testing-framework-summary.md
          echo "- üîÑ **Cross-Validation Tolerance**: 1e-6" >> testing-framework-summary.md
          echo "- üìà **Performance Regression Threshold**: 5%" >> testing-framework-summary.md
          echo "- ‚ö° **Test Execution Time**: <15 minutes (target)" >> testing-framework-summary.md

          if [[ -d "workflow-artifacts" ]]; then
            echo "" >> testing-framework-summary.md
            echo "## Available Artifacts" >> testing-framework-summary.md
            echo "" >> testing-framework-summary.md
            
            artifact_count=$(find workflow-artifacts -type d -name "*" | wc -l)
            echo "- **Total Artifacts**: $artifact_count artifact sets" >> testing-framework-summary.md
            echo "- **Retention**: 30-90 days depending on type" >> testing-framework-summary.md
            echo "- **Contents**: Test results, coverage reports, performance data, logs" >> testing-framework-summary.md
          fi

          echo "" >> testing-framework-summary.md
          echo "## Overall Status" >> testing-framework-summary.md
          echo "" >> testing-framework-summary.md

          # Determine overall status
          CORE_FAILED=false
          OPTIONAL_FAILED=false

          # Check core workflows
          if [[ "${{ needs.unit-tests.result }}" == "failure" ]] || \
             [[ "${{ needs.integration-tests.result }}" == "failure" ]] || \
             [[ "${{ needs.coverage-collection.result }}" == "failure" ]]; then
            CORE_FAILED=true
          fi

          # Check optional workflows
          if [[ "${{ needs.cross-validation.result }}" == "failure" ]] || \
             [[ "${{ needs.performance-benchmarks.result }}" == "failure" ]]; then
            OPTIONAL_FAILED=true
          fi

          if [[ "$CORE_FAILED" == "true" ]]; then
            echo "‚ùå **Overall Status**: FAILED - Core testing workflows failed" >> testing-framework-summary.md
            echo "" >> testing-framework-summary.md
            echo "**Action Required**: Address core test failures before merging." >> testing-framework-summary.md
          elif [[ "$OPTIONAL_FAILED" == "true" ]]; then
            echo "‚ö†Ô∏è **Overall Status**: WARNING - Optional workflows failed" >> testing-framework-summary.md
            echo "" >> testing-framework-summary.md
            echo "**Recommendation**: Review optional workflow failures for potential issues." >> testing-framework-summary.md
          else
            echo "‚úÖ **Overall Status**: SUCCESS - All executed workflows passed" >> testing-framework-summary.md
            echo "" >> testing-framework-summary.md
            echo "**Result**: Testing framework execution completed successfully." >> testing-framework-summary.md
          fi

      - name: Upload comprehensive summary
        uses: actions/upload-artifact@v4
        with:
          name: testing-framework-summary
          path: testing-framework-summary.md
          retention-days: 90

      - name: Create status check
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            // Determine overall status
            const coreResults = [
              '${{ needs.unit-tests.result }}',
              '${{ needs.integration-tests.result }}',
              '${{ needs.coverage-collection.result }}'
            ];

            const optionalResults = [
              '${{ needs.cross-validation.result }}',
              '${{ needs.performance-benchmarks.result }}'
            ];

            const coreFailures = coreResults.filter(r => r === 'failure').length;
            const optionalFailures = optionalResults.filter(r => r === 'failure').length;

            let state, description;
            if (coreFailures > 0) {
              state = 'failure';
              description = `Core testing failed (${coreFailures} failures)`;
            } else if (optionalFailures > 0) {
              state = 'success'; // Don't fail on optional workflow failures
              description = `Core tests passed, ${optionalFailures} optional failures`;
            } else {
              state = 'success';
              description = 'All testing framework workflows passed';
            }

            await github.rest.repos.createCommitStatus({
              owner: context.repo.owner,
              repo: context.repo.repo,
              sha: context.sha,
              state: state,
              target_url: `${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}`,
              description: description,
              context: 'bitnet-rs/testing-framework'
            });

      - name: Comment on PR with summary
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');

            if (fs.existsSync('testing-framework-summary.md')) {
              const summary = fs.readFileSync('testing-framework-summary.md', 'utf8');
              
              // Add header for PR comment
              const prComment = `<!-- BitNet.rs Testing Framework Summary -->
            ${summary}

            ---
            *This comment was generated by the Testing Framework Master Workflow.*`;

              // Find existing comment
              const comments = await github.rest.issues.listComments({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
              });
              
              const botComment = comments.data.find(comment => 
                comment.user.type === 'Bot' && 
                comment.body.includes('<!-- BitNet.rs Testing Framework Summary -->')
              );
              
              if (botComment) {
                // Update existing comment
                await github.rest.issues.updateComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  comment_id: botComment.id,
                  body: prComment
                });
              } else {
                // Create new comment
                await github.rest.issues.createComment({
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  issue_number: context.issue.number,
                  body: prComment
                });
              }
            }

      - name: Determine final status
        run: |
          # Check core workflow results
          CORE_FAILED=false
          if [[ "${{ needs.unit-tests.result }}" == "failure" ]] || \
             [[ "${{ needs.integration-tests.result }}" == "failure" ]] || \
             [[ "${{ needs.coverage-collection.result }}" == "failure" ]]; then
            CORE_FAILED=true
          fi

          if [[ "$CORE_FAILED" == "true" ]]; then
            echo "‚ùå Testing Framework execution failed - core workflows failed"
            exit 1
          else
            echo "‚úÖ Testing Framework execution completed successfully"
          fi
