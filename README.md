# bitnet-rs

[![CI](https://github.com/EffortlessMetrics/BitNet-rs/actions/workflows/ci-core.yml/badge.svg?branch=main)](https://github.com/EffortlessMetrics/BitNet-rs/actions/workflows/ci-core.yml)
[![MSRV](https://img.shields.io/badge/MSRV-1.92.0-blue.svg)](./rust-toolchain.toml)
[![Rust 2024](https://img.shields.io/badge/edition-2024-orange.svg)](./rust-toolchain.toml)
[![License](https://img.shields.io/badge/license-MIT%2FApache--2.0-blue.svg)](./LICENSE)

BitNet-rs is a high-performance Rust inference engine for 1-bit BitNet LLMs.

## Features

- **SIMD/CUDA/Metal/Vulkan kernels** ‚Äî AVX2/AVX-512/NEON on CPU; CUDA (`gpu`), Metal (`metal`, macOS), Vulkan (`vulkan`), Intel oneAPI (`oneapi`) GPU backends
- **Multiple quantization formats** ‚Äî I2_S BitNet32-F16, I2_S QK256 (GGML 256-element blocks), TL1, TL2, IQ2_S via FFI
- **Cross-validation** ‚Äî per-token cosine-similarity comparison against Microsoft's C++ reference (>0.99)
- **Honest-compute receipts** ‚Äî schema v1.0.0 with 8 validation gates; `compute_path` must be `"real"`
- **Chat templates** ‚Äî 53 prompt templates covering 48+ model families with auto-detection from GGUF metadata or tokenizer path; shared ChatML helpers reduce duplication; `PromptTemplate::all_variants()` enumerates all templates
- **SafeTensors ‚Üí GGUF export** ‚Äî `bitnet-st2gguf` preserves F16 LayerNorm weights
- **Multi-SLM architecture registry** ‚Äî 80+ architecture strings across 48+ model families with auto-detected normalization, activation, and context defaults

<details>
<summary><strong>Supported Model Architectures</strong> (click to expand)</summary>

| Family | Architectures | Norm | Activation | Template | Tokenizer |
|--------|--------------|------|------------|----------|-----------|
| BitNet | bitnet, bitnet-b1.58 | LayerNorm | ReLU¬≤ | Instruct | bitnet_custom |
| LLaMA | llama | RmsNorm | SiLU | Llama3Chat | llama3_128k |
| Phi | phi, phi-3, phi-4 | RmsNorm | SiLU | Phi4Chat | phi4_100k |
| Qwen | qwen, qwen2, qwen2.5 | RmsNorm | SiLU | QwenChat | qwen2_150k |
| Gemma | gemma, gemma2 | RmsNorm | GeLU | GemmaChat | gemma_256k |
| Mistral | mistral | RmsNorm | SiLU | MistralChat | mistral_32k |
| DeepSeek | deepseek, deepseek2 | RmsNorm | SiLU | DeepSeekChat | deepseek_100k |
| StarCoder | starcoder, starcoder2 | LayerNorm | GeLU | StarCoder | starcoder_49k |
| Falcon | falcon | LayerNorm | GeLU | FalconChat | falcon_65k |
| CodeLlama | codellama, code-llama | RmsNorm | SiLU | CodeLlamaInstruct | codellama_32k |
| Cohere | command, command-r, cohere | RmsNorm | SiLU | CohereCommand | command_256k |
| InternLM | internlm, internlm2 | RmsNorm | SiLU | InternLMChat | internlm_103k |
| Yi | yi, yi-1.5 | RmsNorm | SiLU | YiChat | yi_64k |
| Baichuan | baichuan, baichuan2 | RmsNorm | SiLU | BaichuanChat | baichuan_64k |
| ChatGLM | chatglm, chatglm2, chatglm3, glm-4 | RmsNorm | SiLU | ChatGLMChat | chatglm_65k |
| MPT | mpt | LayerNorm | GeLU | MptInstruct | mpt_50k |
| RWKV | rwkv, rwkv5, rwkv6 | LayerNorm | SiLU | RwkvWorld | rwkv_65k |
| OLMo | olmo, olmo2 | LayerNorm/RmsNorm | SiLU | OlmoInstruct | olmo_50k |
| Zephyr | zephyr | RmsNorm | SiLU | ZephyrChat | zephyr_32k |
| Vicuna | vicuna | RmsNorm | SiLU | VicunaChat | vicuna_32k |
| Orca | orca | RmsNorm | SiLU | OrcaChat | orca_32k |
| Solar | solar | RmsNorm | SiLU | SolarInstruct | solar_32k |
| Alpaca | alpaca | RmsNorm | SiLU | AlpacaInstruct | alpaca_32k |
| Command-R+ | command-r, command-r-plus | RmsNorm | SiLU | CommandRPlus | commandr_128k |
| Nous Hermes | nous-hermes, hermes | RmsNorm | SiLU | NousHermes | nous_32k |
| WizardLM | wizardlm, wizard | RmsNorm | SiLU | WizardLM | wizard_32k |
| OpenChat | openchat | RmsNorm | SiLU | OpenChat | openchat_32k |
| Granite | granite | RmsNorm | SiLU | GraniteChat | granite_128k |
| Nemotron | nemotron | RmsNorm | SiLU | NemotronChat | nemotron_32k |
| Saiga | saiga | RmsNorm | SiLU | SaigaChat | saiga_32k |
| Llama-2 | llama2, llama-2 | RmsNorm | SiLU | Llama2Chat | llama2_32k |
| Gemma 2 | gemma2, gemma-2 | RmsNorm | GeLU | Gemma2Chat | gemma2_256k |
| Phi-3 | phi3, phi-3 | RmsNorm | SiLU | Phi3Instruct | phi3_32k |
| Llama-2 | llama2, llama-2 | RmsNorm | SiLU | Llama2Chat | llama2_32k |
| Gemma 2 | gemma2, gemma-2 | RmsNorm | GeLU | Gemma2Chat | gemma2_256k |
| Mixtral | mixtral | RmsNorm | SiLU | MixtralInstruct | mixtral_32k |
| Mistral Nemo | mistral-nemo, nemo | RmsNorm | SiLU | MistralNemoChat | mistral_nemo_128k |
| Qwen 2.5 | qwen2.5, qwen-2.5 | RmsNorm | SiLU | Qwen25Chat | qwen25_152k |
| TinyLlama | tinyllama | RmsNorm | SiLU | TinyLlamaChat | tinyllama_32k |
| Dolphin | dolphin | RmsNorm | SiLU | DolphinChat | dolphin_32k |
| ChatGPT | chatgpt, gpt4, gpt-4 | LayerNorm | GeLU | ChatGptChat | chatgpt_100k |
| StableLM | stablelm, stable-lm | RmsNorm | SiLU | StableLMChat | stablelm_32k |
| Bloom | bloom, bloomz | LayerNorm | GeLU | BloomChat | bloom_250k |
| Jamba | jamba | RmsNorm | SiLU | JambaChat | jamba_256k |
| Persimmon | persimmon, adept | LayerNorm | GeLU | PersimmonChat | persimmon_262k |
| XVERSE | xverse | RmsNorm | SiLU | XverseChat | xverse_32k |
| Arctic | arctic | RmsNorm | SiLU | ArcticInstruct | arctic_32k |
| DBRX | dbrx | RmsNorm | SiLU | DbrxInstruct | dbrx_32k |
| EXAONE | exaone | RmsNorm | SiLU | ExaoneChat | exaone_32k |
| MiniCPM | minicpm | RmsNorm | SiLU | MiniCPMChat | minicpm_122k |
| CodeGemma | codegemma, code-gemma | RmsNorm | GeLU | CodeGemma | codegemma_256k |
| Llama 3.1 | llama-3.1, llama3.1, llama31 | RmsNorm | SiLU | Llama31Chat | llama31_128k |
| DeepSeek V3 | deepseek-v3, deepseekv3, deepseek3 | RmsNorm | SiLU | DeepSeekV3Chat | deepseekv3_100k |
| GPT | gpt | LayerNorm | GeLU | ‚Äî | gpt2_50k |
| BERT | bert | LayerNorm | GeLU | ‚Äî | ‚Äî |

</details>

> **v0.2.0:** QK256 uses scalar kernels (~0.1 tok/s on 2B models); use `--max-tokens 4‚Äì16` for validation. AVX2 dequantization is merged; ‚â•3√ó uplift planned for v0.2.

## Quick Start

```bash
# 1. Download a model
cargo run -p xtask -- download-model --id microsoft/bitnet-b1.58-2B-4T-gguf

# 2. Run inference  (always specify --no-default-features --features cpu|gpu)
RUST_LOG=warn cargo run -p bitnet-cli --no-default-features --features cpu,full-cli -- run \
  --model  models/microsoft-bitnet-b1.58-2B-4T-gguf/ggml-model-i2_s.gguf \
  --tokenizer models/microsoft-bitnet-b1.58-2B-4T-gguf/tokenizer.json \
  --prompt "What is 2+2?" --max-tokens 8

# 3. Interactive chat
RUST_LOG=warn cargo run -p bitnet-cli --no-default-features --features cpu,full-cli -- chat \
  --model  models/microsoft-bitnet-b1.58-2B-4T-gguf/ggml-model-i2_s.gguf \
  --tokenizer models/microsoft-bitnet-b1.58-2B-4T-gguf/tokenizer.json
```

> Default features are **empty** by design ‚Äî always pass `--no-default-features --features cpu` (or `gpu`).

## Status

| Feature                        | State | Notes |
|-------------------------------|-------|-------|
| CPU inference ‚Äî I2_S BitNet32 | ‚úÖ    | Production path; 10‚Äì20√ó faster than QK256 scalar |
| CPU inference ‚Äî I2_S QK256    | ‚úÖ    | Scalar kernels (~0.1 tok/s on 2B); AVX2 foundation merged |
| GPU inference ‚Äî CUDA          | ‚ö†Ô∏è   | Implemented; receipt validation pending |
| GPU inference ‚Äî Metal         | ‚úÖ    | macOS/iOS via `--features metal` (#992) |
| GPU inference ‚Äî Vulkan        | ‚úÖ    | Runtime probing via `--features vulkan` (#993) |
| GPU inference ‚Äî Intel oneAPI  | ‚úÖ    | Intel CPU/GPU via `--features oneapi` (#986) |
| AMD ROCm detection            | ‚úÖ    | `rocm_available` field in `DeviceProbe` (#995) |
| Interactive chat (REPL)       | ‚úÖ    | `/help`, `/clear`, `/metrics`, auto-template detection |
| Cross-validation vs C++       | ‚úÖ    | Cosine similarity > 0.99, per-token comparison |
| Honest-compute receipts       | ‚úÖ    | Schema v1.0.0, 8 validation gates |
| Strict mode                   | ‚úÖ    | Runtime guards prevent mock fallback |
| SafeTensors ‚Üí GGUF export     | ‚úÖ    | `bitnet-st2gguf` with F16 LayerNorm preservation |
| Server / HTTP API             | üöß    | Health endpoints wired; inference endpoints have TODOs |

## Architecture

Data flows top-to-bottom through the workspace:

```
bitnet-tokenizers ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                                         ‚îÇ
bitnet-models  (GGUF loader, dual I2_S flavor detection) ‚îÇ
  ‚îî‚îÄ‚îÄ bitnet-quantization  (I2_S / TL1 / TL2 / IQ2_S)  ‚îÇ
        ‚îî‚îÄ‚îÄ bitnet-kernels (AVX2 / AVX-512 / NEON / CUDA)‚îÇ
                                                         ‚ñº
                        bitnet-inference  (autoregressive engine)
                          ‚îú‚îÄ‚îÄ bitnet-logits       (temperature / top-k / top-p)
                          ‚îú‚îÄ‚îÄ bitnet-sampling     (greedy, nucleus, repetition penalty)
                          ‚îú‚îÄ‚îÄ bitnet-generation   (decode loop, stop criteria)
                          ‚îú‚îÄ‚îÄ bitnet-prompt-templates  (raw / instruct / llama3-chat)
                          ‚îî‚îÄ‚îÄ bitnet-receipts     (honest-compute receipt schema)
                                                         ‚îÇ
                                          ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                                     bitnet-cli                  bitnet-server
```

**SRP microcrates** (`bitnet-logits`, `bitnet-sampling`, `bitnet-generation`, `bitnet-engine-core`, `bitnet-device-probe`, `bitnet-gguf`, `bitnet-prompt-templates`, `bitnet-receipts`) keep coupling low and are re-exported from their original locations for zero breaking changes.

## Documentation

Organised by [Di√°taxis](https://diataxis.fr/):

| Section | Contents |
|---------|----------|
| [**Tutorials**](docs/tutorials/) | Getting started, first inference, tokenizer discovery |
| [**How-to**](docs/howto/) | Install, run inference, export GGUF, cross-validate, validate models |
| [**Explanation**](docs/explanation/) | Architecture, quantization formats, dual-backend cross-val, feature flags |
| [**Reference**](docs/reference/) | CLI flags, environment variables, API, quantization support |

Key guides: [Quickstart](docs/quickstart.md) ¬∑ [Environment variables](docs/environment-variables.md) ¬∑ [GPU setup](docs/GPU_SETUP.md) ¬∑ [C++ cross-validation](docs/howto/cpp-setup.md) ¬∑ [Quantization support](docs/reference/quantization-support.md) ¬∑ [Validation gates](docs/reference/validation-gates.md) ¬∑ [Honest-compute receipts](docs/howto/receipt-verification.md) ¬∑ [QK256 usage](docs/howto/use-qk256-models.md) ¬∑ [macOS 26 Apple Silicon roadmap](docs/reference/macos-26-apple-silicon-roadmap.md)

## Building

```bash
cargo build --no-default-features --features cpu           # CPU (development)
cargo build --no-default-features --features gpu           # GPU (requires CUDA 12.x)
RUSTFLAGS="-C target-cpu=native -C opt-level=3 -C lto=thin" \
  cargo build --release --no-default-features --features cpu,full-cli  # optimised release

# Nix (reproducible, identical to CI)
nix develop && nix build .#bitnet-cli && nix flake check
```

### Feature flags

| Flag | Purpose |
|------|---------|
| `cpu` | SIMD-optimised CPU inference (AVX2 / AVX-512 / NEON) |
| `gpu` | Umbrella GPU feature ‚Äî enables all compiled GPU backends |
| `cuda` | CUDA acceleration (preferred; requires CUDA 12.x); backward-compat alias for `gpu` |
| `metal` | Metal GPU backend (macOS/iOS Apple Silicon) |
| `vulkan` | Vulkan compute backend (cross-platform) |
| `oneapi` | Intel oneAPI backend (Intel CPU/GPU) |
| `ffi` | C++ FFI bridge for cross-validation |
| `fixtures` | GGUF fixture-based integration tests (test-only) |
| `full-cli` | Enable all CLI subcommands |

Always use the unified GPU predicate in Rust code:
```rust
#[cfg(any(feature = "gpu", feature = "cuda"))]
```

## Testing

```bash
# Run all enabled tests (recommended ‚Äî 5-minute timeout)
cargo nextest run --workspace --no-default-features --features cpu

# CI profile (4 threads, no retries)
cargo nextest run --profile ci

# Skip slow QK256 scalar-kernel tests
BITNET_SKIP_SLOW_TESTS=1 cargo nextest run --workspace --no-default-features --features cpu

# BDD compile-coverage check
cargo run -p xtask -- grid-check

# Fixture-based integration tests
cargo test -p bitnet-models --test qk256_dual_flavor_tests --no-default-features --features fixtures

# Lint before pushing
cargo fmt --all && cargo clippy --all-targets --no-default-features --features cpu -- -D warnings
```

The suite has 1000+ enabled tests spanning unit, property-based (proptest), snapshot (insta), fixture, fuzz (13 targets), and BDD grid categories. ~70 tests are intentionally `#[ignore]`-d (TDD scaffolding for issues #254, #260, #469 ‚Äî not failures to fix).

See [docs/development/test-suite.md](docs/development/test-suite.md) for full details.

## Contributing

See [CONTRIBUTING.md](CONTRIBUTING.md). Issues and pull requests welcome.

Before opening a PR, run:
```bash
cargo fmt --all && cargo clippy --all-targets --no-default-features --features cpu -- -D warnings
cargo nextest run --workspace --no-default-features --features cpu
```

Note: ~70 tests are intentionally `#[ignore]`-d (TDD scaffolding for tracked issues #254, #260, #469). This is expected MVP behaviour, not failures to fix.

## License

Dual-licensed under [MIT](LICENSE) and [Apache 2.0](LICENSE).
