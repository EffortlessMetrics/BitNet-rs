# BitNet.rs - Production-Ready 1-bit LLM Inference

[![Crates.io](https://img.shields.io/crates/v/bitnet.svg)](https://crates.io/crates/bitnet)
[![Documentation](https://docs.rs/bitnet/badge.svg)](https://docs.rs/bitnet)
[![License](https://img.shields.io/badge/license-MIT%20OR%20Apache--2.0-blue.svg)](https://github.com/microsoft/BitNet#license)
[![Build Status](https://github.com/microsoft/BitNet/workflows/CI/badge.svg)](https://github.com/microsoft/BitNet/actions)
[![MSRV](https://img.shields.io/badge/MSRV-1.90.0-blue.svg)](https://github.com/microsoft/BitNet)

**High-performance Rust implementation of BitNet 1-bit Large Language Model inference**
with memory safety, device-aware quantization, and cross-platform support.

> ‚úÖ **Validated Drop-in Replacement**: Compatible with llama.cpp, handles models that crash C++ diagnostic tools.

## Why BitNet.rs?

- üöÄ **High Performance**: Real quantized inference (10-20 tok/s CPU, 50-100 tok/s GPU),
  SIMD kernels (AVX2/AVX-512/NEON), CUDA acceleration
- üõ°Ô∏è **Memory Safe**: No segfaults or leaks, comprehensive error handling, strict mode
  prevents mock fallbacks
- üåê **Cross-Platform**: Linux/macOS/Windows, CPU/GPU backends, I2S/TL1/TL2 quantization
- üîß **Developer Friendly**: Modern Rust tooling, extensive documentation,
  cross-validation against C++ reference

## Quick Start

### Installation

```bash
# Rust library
cargo add bitnet

# CLI tools
cargo install bitnet-cli bitnet-server

# Python bindings
pip install bitnet-rs
```

### Basic Usage

```bash
# Build and test (CPU with real quantization)
cargo build --no-default-features --features cpu
cargo test --workspace --no-default-features --features cpu

# GPU support with mixed precision
cargo build --no-default-features --features gpu

# Download and run inference with strict mode (prevents mock fallbacks)
cargo run -p xtask -- download-model
BITNET_STRICT_MODE=1 cargo run -p xtask -- infer --model path/to/model.gguf --prompt "Hello"
```

### CLI Quickstart

BitNet.rs supports three main inference modes:

| Use Case | Command Example | Description |
|----------|----------------|-------------|
| **Deterministic Q&A** | `bitnet run --model model.gguf --tokenizer tokenizer.json --prompt "What is 2+2?" --max-tokens 16 --temperature 0.0` | Reproducible answers with greedy decoding |
| **Creative Completion** | `bitnet run --model model.gguf --tokenizer tokenizer.json --prompt "Explain photosynthesis" --max-tokens 128 --temperature 0.7 --top-p 0.95` | Nucleus sampling for natural text generation |
| **Interactive Chat** | `bitnet chat --model model.gguf --tokenizer tokenizer.json` | REPL with auto-detected templates and streaming |

**Template defaults:** The CLI now defaults to `--prompt-template auto`, which detects a
suitable template from GGUF/HF metadata. To preserve legacy behavior, pass
`--prompt-template raw`.

**Note**: Use `--no-default-features --features cpu` for CPU-only builds, or
`--no-default-features --features gpu` for CUDA acceleration.

**CLI Interface Version**: 1.0.0 ‚Äî Use `bitnet --interface-version` to check compatibility.

### Deterministic Math Sanity Check

For reproducible Q&A testing (validates model correctness):

```bash
# Greedy math sanity check with reduced log noise
RUST_LOG=warn cargo run -p bitnet-cli --no-default-features --features cpu,full-cli -- run \
  --model models/microsoft-bitnet-b1.58-2B-4T-gguf/ggml-model-i2_s.gguf \
  --tokenizer models/microsoft-bitnet-b1.58-2B-4T-gguf/tokenizer.json \
  --prompt "Answer with a single digit: 2+2=" \
  --max-tokens 1 \
  --temperature 0.0 \
  --greedy

# Expected output: "4"
# Use for: deterministic validation, receipt generation, CI testing
# Environment: RUST_LOG=warn suppresses verbose logging
```

### Q&A with Instruct Template

```bash
# Auto-detect template and generate answer (recommended)
RUST_LOG=warn cargo run -p bitnet-cli --no-default-features --features cpu,full-cli -- run \
  --model models/model.gguf \
  --tokenizer models/tokenizer.json \
  --prompt "Who wrote 'Pride and Prejudice'?" \
  --max-tokens 128

# Explicit instruct template
RUST_LOG=warn cargo run -p bitnet-cli --no-default-features --features cpu,full-cli -- run \
  --model models/model.gguf \
  --tokenizer models/tokenizer.json \
  --prompt-template instruct \
  --prompt "What is the capital of France?" \
  --max-tokens 64 \
  --temperature 0.7 --top-p 0.95
```

### LLaMA-3 Chat

```bash
# LLaMA-3 chat format with system prompt
cargo run -p bitnet-cli --no-default-features --features cpu,full-cli -- run \
  --model models/llama3-model.gguf \
  --tokenizer models/tokenizer.json \
  --prompt-template llama3-chat \
  --system-prompt "You are a helpful assistant" \
  --prompt "Explain photosynthesis" \
  --max-tokens 128 \
  --temperature 0.7 \
  --top-p 0.95 \
  --stop "<|eot_id|>"
```

### Interactive Chat

```bash
# Auto-detect template and start interactive chat with clean output
RUST_LOG=warn cargo run -p bitnet-cli --no-default-features --features cpu,full-cli -- chat \
  --model models/model.gguf \
  --tokenizer models/tokenizer.json \
  --temperature 0.7 --top-p 0.95
```

**Note**: The CLI automatically detects the appropriate prompt template from GGUF metadata
and model paths. You can override with `--prompt-template` (options: auto, raw, instruct,
llama3-chat). Auto-detection defaults to `instruct` for better out-of-box Q&A performance.

**Tip**: Use `RUST_LOG=warn` to reduce log noise and focus on generated text in all examples.

### Using QK256 Models

BitNet.rs supports GGML-compatible QK256 I2_S models with pure-Rust inference (no C++
dependencies required). This section shows how to download, validate, and run inference
with QK256 models.

#### QK256 Setup

```bash
# 1. Build with CPU support (includes QK256 kernels)
cargo build --release --no-default-features --features cpu

# 2. Download Microsoft QK256 model
cargo run -p xtask -- download-model --id microsoft/bitnet-b1.58-2B-4T-gguf

# 3. Fetch LLaMA-3 tokenizer (required for this model)
# Option A: Official source (requires HF_TOKEN with LLaMA-3 license accepted)
HF_TOKEN=your_token cargo run -p xtask -- tokenizer \
  --into models/microsoft-bitnet-b1.58-2B-4T-gguf \
  --source official

# Option B: Mirror source (no authentication, development use)
cargo run -p xtask -- tokenizer \
  --into models/microsoft-bitnet-b1.58-2B-4T-gguf \
  --source mirror

# 4. Run inference with strict loader (ensures QK256 format is properly loaded)
cargo run -p bitnet-cli --no-default-features --features cpu,full-cli -- run \
  --model models/microsoft-bitnet-b1.58-2B-4T-gguf/ggml-model-i2_s.gguf \
  --prompt "What is 2+2?" \
  --max-tokens 16

# 5. Run parity smoke test (validates against C++ reference if available)
scripts/parity_smoke.sh \
  models/microsoft-bitnet-b1.58-2B-4T-gguf/ggml-model-i2_s.gguf \
  models/microsoft-bitnet-b1.58-2B-4T-gguf/tokenizer.json
```

**Receipt Location:** Parity validation writes receipts to
`docs/baselines/<YYYY-MM-DD>/parity-bitnetcpp.json` with cosine similarity,
exact match rate, and kernel IDs.

#### I2_S Quantization Flavors

BitNet.rs automatically detects the I2_S quantization flavor based on tensor size:

| Format | Block Size | Scales | Support Status | Use Case |
|--------|-----------|--------|-----------------|----------|
| **I2_S BitNet32-F16** | 32 elements | Inline F16 | üöß Production (CPU/GPU) | Microsoft BitNet native models |
| **I2_S QK256 (GGML)** | 256 elements | Separate tensor | ‚úÖ MVP (scalar), üöß SIMD | GGML-compatible models |
| **TL1** | 4-bit blocks | LUT entries | ‚ö† ARM NEON optimized | ARM-based inference |
| **TL2** | 8-bit blocks | LUT entries | ‚ö† x86 AVX2/AVX-512 optimized | x86-based inference |

**Key Differences:**

- **BitNet32-F16**: 32-element blocks with inline f16 scales (10 bytes/block) - optimized for BitNet models
- **QK256 (GGML)**: 256-element blocks with separate scale tensor (64 bytes/block) - compatible with GGML ecosystem

#### Deterministic Inference

For reproducible results across runs (e.g., testing, validation, receipts):

```bash
# Set environment variables for deterministic inference
export BITNET_DETERMINISTIC=1   # Enable deterministic mode
export BITNET_SEED=42            # Fixed random seed
export RAYON_NUM_THREADS=1       # Single-threaded execution

# Run inference (output will be identical across runs)
cargo run -p bitnet-cli --no-default-features --features cpu,full-cli -- run \
  --model models/microsoft-bitnet-b1.58-2B-4T-gguf/ggml-model-i2_s.gguf \
  --prompt "What is 2+2?" \
  --max-tokens 16 \
  --seed 42
```

**Note:** Deterministic mode ensures reproducible token sequences and logits for validation workflows.

#### QK256 Deterministic Sanity Testing

For QK256 models, you can run a deterministic 1-token sanity test:

```bash
# QK256 deterministic sanity (scalar MVP; 1 token)
BITNET_DETERMINISTIC=1 RAYON_NUM_THREADS=1 RUST_LOG=warn \
bitnet run \
  --model models/microsoft-bitnet-b1.58-2B-4T-gguf/ggml-model-i2_s.gguf \
  --tokenizer models/microsoft-bitnet-b1.58-2B-4T-gguf/tokenizer.json \
  --prompt "x" --temperature 0.0 --top-k 0 --top-p 1.0 --max-new-tokens 1
```

**Note:** The QK256 MVP uses a scalar kernel; parity testing in CI uses 1-token
inference intentionally for speed. Longer runs will be slower than bitnet.cpp
until SIMD optimizations land.

#### Learn More

- **Comprehensive Guide:**
  [How to Use QK256 Models](docs/howto/use-qk256-models.md) - Detailed QK256
  usage, troubleshooting, and advanced workflows
- **Architecture Deep Dive:**
  [I2_S Dual Flavor Architecture](docs/explanation/i2s-dual-flavor.md) -
  Technical specifications and implementation details
- **Quick Start:** [5-Minute Quick Start](docs/quickstart.md) - General BitNet.rs setup guide

#### Flag Aliases for Compatibility

BitNet.rs CLI provides aliases for common flags to maintain compatibility with other tools:

```bash
# These are equivalent (primary flag: --max-tokens)
bitnet run --max-tokens 32        # Primary flag
bitnet run --max-new-tokens 32    # Alias (common in other tools)
bitnet run --n-predict 32         # Alias (GGML compatibility)

# These are equivalent (primary flag: --stop)
bitnet run --stop "</s>"          # Primary flag
bitnet run --stop-sequence "</s>" # Alias
bitnet run --stop_sequences "</s>" # Alias
```

### CPU Performance Optimization

For maximum CPU inference throughput on your hardware:

```bash
# Build with native CPU optimization (recommended for production)
RUSTFLAGS="-C target-cpu=native -C opt-level=3 -C lto=thin" \
  cargo build --release --no-default-features --features cpu,full-cli

# Run with full CPU parallelization
RAYON_NUM_THREADS=$(nproc) RUST_LOG=warn \
  cargo run --release -p bitnet-cli --no-default-features --features cpu,full-cli -- run \
  --model models/model.gguf \
  --tokenizer models/tokenizer.json \
  --prompt "Explain photosynthesis" \
  --max-tokens 128 \
  --temperature 0.7 --top-p 0.95

# For deterministic benchmarks (single-threaded, reproducible results)
RUSTFLAGS="-C target-cpu=native -C opt-level=3 -C lto=thin" \
  cargo build --release --no-default-features --features cpu,full-cli
RAYON_NUM_THREADS=1 RUST_LOG=warn \
  cargo run --release -p bitnet-cli --no-default-features --features cpu,full-cli -- run \
  --model models/model.gguf \
  --tokenizer models/tokenizer.json \
  --prompt "Test" --max-tokens 32 --greedy
```

**Performance Tuning:**

- `target-cpu=native`: Enable all CPU instructions available on your machine (AVX2/AVX-512/NEON)
- `opt-level=3`: Maximum optimization (aggressive inlining, vectorization)
- `lto=thin`: Link-time optimization for performance without excessive build time
- `RAYON_NUM_THREADS=$(nproc)`: Use all CPU cores for parallelization
- `RUST_LOG=warn`: Reduce logging overhead during inference

**Expected Performance:** 20-100 tok/s CPU (depends on hardware, model size, optimization level).

### Receipt Verification Workflow

Generate, verify, and pin performance baselines:

```bash
# Receipts: run ‚Üí emit ‚Üí verify (with CPU optimization)
export BITNET_STRICT_MODE=1 BITNET_DETERMINISTIC=1 BITNET_SEED=42 RAYON_NUM_THREADS=1
RUSTFLAGS="-C target-cpu=native -C opt-level=3" \
  cargo build --release --no-default-features --features cpu
cargo run -p xtask -- benchmark --model tests/models/tiny.gguf --tokens 128 --deterministic
cargo run -p xtask -- verify-receipt ci/inference.json
mkdir -p docs/baselines && cp ci/inference.json docs/baselines/$(date +%Y%m%d)-cpu.json
```

**Expected Performance:** 10-20 tok/s on CPU for 2B I2_S models (see
[docs/baselines/](docs/baselines/) for measured results).

**Receipt Verification:** All inference runs generate receipts (`ci/inference.json`) with
kernel IDs proving real computation. CI blocks PRs with mocked receipts.

### Rust API

```rust
use bitnet::prelude::*;

#[tokio::main]
async fn main() -> Result<()> {
    // Load GGUF model with real quantized weights
    let model = BitNetModel::from_file("model.gguf").await?;

    // Create inference engine with device auto-detection and strict mode
    std::env::set_var("BITNET_STRICT_MODE", "1");  // Prevent mock fallbacks
    let engine = InferenceEngine::builder()
        .model(model)
        .backend(Backend::Auto)  // GPU if available, CPU fallback
        .quantization(QuantizationType::I2S)  // Real quantized computation
        .build()?;

    // Generate text with real neural network inference
    let response = engine.generate("Explain quantum computing").await?;
    println!("Generated: {}", response.text);

    // Access realistic performance metrics
    if let Some(metrics) = response.metrics {
        println!("Throughput: {:.1} tokens/sec", metrics.throughput.e2e);
        println!("Quantization: I2S (99.8% accuracy vs FP32)");
    }

    Ok(())
}
```

## Core Features

### Quantization Support

BitNet.rs supports multiple quantization formats with automatic detection and
device-aware kernels. See [Using QK256 Models](#using-qk256-models) for
detailed I2_S flavor comparison.

**Key Features:**

- **I2_S**: Production 2-bit signed quantization (‚â•99.8% accuracy vs FP32, 10-20 tok/s CPU, 50-100 tok/s GPU)
  - **BitNet32-F16**: Native BitNet format with inline F16 scales (32-element blocks)
  - **QK256 (GGML)**: GGML-compatible format with separate scale tensor (256-element blocks)
  - **Automatic Flavor Detection**: Model loader identifies quantization flavor from tensor size
- **TL1/TL2**: Table lookup quantization with device-aware selection (‚â•99.6% accuracy vs FP32)
- **Real Computation**: Native quantized matrix multiplication eliminates mock fallbacks
- **Strict Mode**: `BITNET_STRICT_MODE=1` ensures production-ready inference paths

### Device-Aware Computing

- Automatic GPU detection and fallback to optimized CPU kernels
- Mixed precision support (FP16/BF16) with Tensor Core acceleration
- Cross-validation against Microsoft BitNet C++ reference (<5% performance variance)
- SIMD acceleration (AVX2/AVX-512/NEON) for CPU inference

### Universal Tokenizer Discovery

- **Automatic Detection**: Extracts tokenizers from GGUF metadata
  (HuggingFace JSON, SentencePiece, vocabulary arrays)
- **Architecture Recognition**: Identifies BitNet, LLaMA-2/3, GPT-2, GPT-Neo,
  BERT, T5 from tensor patterns
- **Tokenizer Fetching**: `cargo run -p xtask -- tokenizer` downloads LLaMA-3
  tokenizers from HuggingFace (official or mirror sources)
- **Auto-Discovery Chain**: Explicit `--tokenizer` ‚Üí GGUF embedded ‚Üí sibling
  `tokenizer.json` ‚Üí parent directory ‚Üí fail with clear error
- **Smart Fallbacks**: Co-located files ‚Üí cache ‚Üí HuggingFace Hub download ‚Üí offline mode
- **Model-Specific Wrappers**: LLaMA (32K/128K variants), GPT-2 (no BOS), BitNet (quantization-aware)
- **Production Mode**: `BITNET_STRICT_TOKENIZERS=1` prevents mock fallbacks
- **Vocabulary Resolution**: 5-strategy extraction from metadata, tensors, or architecture defaults
- **O(1) Performance**: Memory-mapped GGUF parsing, zero-copy tokenizer extraction

## Receipt Verification

BitNet.rs implements **honest compute** verification through production receipts. Every
inference run generates a receipt with kernel IDs proving real computation.

### Receipt Schema v1.0.0

```json
{
  "schema_version": "1.0.0",
  "compute_path": "real",
  "backend": "cpu",
  "model": "microsoft/bitnet-b1.58-2B-4T-gguf",
  "quantization": "i2s",
  "tokens_generated": 128,
  "throughput_tokens_per_sec": 15.3,
  "success": true,
  "kernels": [
    "i2s_cpu_quantized_matmul",
    "tl1_lut_dequant_forward",
    "attention_kv_cache_update",
    "layernorm_forward"
  ],
  "timestamp": "2025-10-15T12:00:00Z"
}
```

### xtask Commands

```bash
# Download tokenizer from HuggingFace
# Official source (requires HF_TOKEN with LLaMA-3 license)
HF_TOKEN=your_token cargo run -p xtask -- tokenizer \
  --into models/model-dir \
  --source official

# Mirror source (no authentication required, development use)
cargo run -p xtask -- tokenizer \
  --into models/model-dir \
  --source mirror

# Force re-download even if tokenizer exists
cargo run -p xtask -- tokenizer --into models/model-dir --force

# Generate receipt (writes ci/inference.json)
cargo run -p xtask -- benchmark --model <model.gguf> --tokens 128

# Verify receipt passes quality gates
cargo run -p xtask -- verify-receipt ci/inference.json

# Strict mode (fail on warnings)
BITNET_STRICT_MODE=1 cargo run -p xtask -- verify-receipt ci/inference.json
```

### Environment Variables

| Variable | Description | Default | Use Case |
|----------|-------------|---------|----------|
| `BITNET_DETERMINISTIC` | Enable deterministic inference | `0` | Reproducible outputs for testing/validation |
| `BITNET_SEED` | Random seed for deterministic mode | `42` | Control randomness in inference |
| `RAYON_NUM_THREADS` | Thread count (use `1` for determinism) | auto | Single-threaded for reproducibility |
| `BITNET_STRICT_MODE` | Fail on validation warnings | `0` | Production deployments |
| `BITNET_GGUF` | Override model path | auto-discover `models/` | Cross-validation workflows |
| `BITNET_CPP_DIR` | Path to C++ reference (parity only) | unset | Parity validation with BitNet.cpp |
| `BITNET_DISABLE_MINIMAL_LOADER` | Fail-fast on loader errors | `0` | CI/CD strict validation |

### Receipt Requirements

**Honest Compute:**

- `compute_path` must be `"real"` (not `"mocked"`)
- `kernels` array must be non-empty
- Kernel IDs must be valid (non-empty, ‚â§128 chars, ‚â§10,000 count)

**CI Enforcement:**

- Model Gates (CPU) workflow requires valid receipts
- Branch protection blocks PRs with mocked receipts
- See [.github/workflows/model-gates.yml](.github/workflows/model-gates.yml)

### Baseline Receipts

Reference receipts are stored in [docs/baselines/](docs/baselines/) with datestamped
filenames (`YYYYMMDD-cpu.json`). These baselines establish reproducible performance
benchmarks for CPU inference.

## Architecture

BitNet.rs is organized as a Rust workspace:

- **`bitnet`**: Main library with unified API
- **`bitnet-inference`**: Autoregressive generation engine
- **`bitnet-quantization`**: 1-bit quantization algorithms (I2_S, TL1, TL2)
- **`bitnet-kernels`**: SIMD/CUDA compute kernels
- **`bitnet-models`**: GGUF/SafeTensors model loading with zero-copy memory mapping
- **`bitnet-tokenizers`**: Universal tokenizer discovery with automatic GGUF extraction

## Documentation

- üìö **[Getting Started](docs/getting-started.md)** - Comprehensive setup guide
- üöÄ **[Quick Start](docs/quickstart.md)** - 5-minute setup
- üèóÔ∏è **[Architecture](docs/architecture-overview.md)** - System design
- üîß **[Development](docs/development/)** - Build, test, and contribution guides
- üìñ **[API Reference](https://docs.rs/bitnet)** - Complete Rust documentation

### Key Guides

- [Tokenizer Discovery](docs/reference/tokenizer-discovery-api.md) - Automatic tokenizer resolution
- [Export Clean GGUF](docs/howto/export-clean-gguf.md) - SafeTensors to GGUF with F16 LayerNorm preservation
- [Model Baselines](docs/baselines/README.md) - Validated model fingerprints and performance metrics
- [GPU Setup](docs/GPU_SETUP.md) - CUDA configuration
- [Performance](docs/performance-benchmarking.md) - Optimization and benchmarking
- [Migration](docs/migration-guide.md) - From C++/Python implementations
- [Troubleshooting](docs/troubleshooting/troubleshooting.md) - Common issues

## Language Bindings

### Python

```python
import bitnet

model = bitnet.BitNetModel("model.gguf")
response = model.generate("Hello, world!")
print(response)
```

### C API

```c
#include "bitnet.h"

BitNetModel* model = bitnet_model_load("model.gguf");
char* response = bitnet_generate(model, "Hello, world!");
printf("%s\n", response);
```

### WebAssembly

```javascript
import init, { BitNetModel } from './pkg/bitnet_wasm.js';

await init();
const model = new BitNetModel('model.gguf');
const response = await model.generate('Hello, world!');
```

## Production Status

‚úÖ **Production Ready** - 100% validation pass rate across all acceptance gates:

- Build, unit tests, tensor mapping, tokenization
- Performance benchmarks, FFI compatibility
- Deterministic outputs, cross-validation

See [VALIDATION.md](VALIDATION.md) for detailed specifications.

## Development

```bash
# Development cycle with strict mode (prevents mock fallbacks)
BITNET_STRICT_MODE=1 cargo test --workspace --no-default-features --features cpu
cargo fmt --all && cargo clippy --all-targets --all-features -- -D warnings

# Cross-validation against C++ reference (validates real quantization)
export BITNET_GGUF="path/to/model.gguf"
BITNET_STRICT_MODE=1 cargo run -p xtask -- crossval
```

See [CONTRIBUTING.md](CONTRIBUTING.md) for contribution guidelines.

## License

This project is licensed under the MIT OR Apache-2.0 license.

## Acknowledgments

- **Microsoft Research** for the original BitNet architecture
- **Rust ML ecosystem** contributors for excellent tooling
- **Community contributors** who make BitNet.rs better every day

---

**MSRV**: 1.90.0 | **Status**: Production Ready | **Performance**: Real quantized
inference (10-20 tok/s CPU, 50-100 tok/s GPU)
