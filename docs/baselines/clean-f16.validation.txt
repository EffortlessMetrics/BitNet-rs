  [2m2025-10-13T19:57:40.792749Z[0m [32m INFO[0m  [32mParsing GGUF file of size 6648803328 bytes with security limits[0m
    [2;3mat[0m crates/bitnet-models/src/formats/gguf/reader.rs:72

  [2m2025-10-13T19:57:40.793576Z[0m [32m INFO[0m  [32mLN gate ruleset: bitnet-b1.58:f16 (architecture: bitnet-b1.58, file_type: 1)[0m
    [2;3mat[0m crates/bitnet-cli/src/commands/inspect.rs:95

model_sha256: ad997658a1d252e748582294bf003b5623e6e401a21dc23402befb7276e8d5aa
ruleset: bitnet-b1.58:f16

model.layers.14.input_layernorm.weight                           [LN]     rms=0.5221   âœ…
model.layers.1.input_layernorm.weight                            [LN]     rms=0.7679   âœ…
model.layers.23.input_layernorm.weight                           [LN]     rms=0.5996   âœ…
model.layers.16.input_layernorm.weight                           [LN]     rms=0.5508   âœ…
model.layers.3.post_attention_layernorm.weight                   [LN]     rms=0.6686   âœ…
model.layers.19.input_layernorm.weight                           [LN]     rms=0.5867   âœ…
model.layers.9.post_attention_layernorm.weight                   [LN]     rms=0.5026   âœ…
model.layers.18.input_layernorm.weight                           [LN]     rms=0.5336   âœ…
model.layers.21.post_attention_layernorm.weight                  [LN]     rms=0.3614   âœ…
model.layers.18.post_attention_layernorm.weight                  [LN]     rms=0.3901   âœ…
model.layers.7.post_attention_layernorm.weight                   [LN]     rms=0.5836   âœ…
model.layers.17.input_layernorm.weight                           [LN]     rms=0.5500   âœ…
model.layers.25.input_layernorm.weight                           [LN]     rms=0.5278   âœ…
model.layers.16.post_attention_layernorm.weight                  [LN]     rms=0.3579   âœ…
model.layers.21.input_layernorm.weight                           [LN]     rms=0.5971   âœ…
model.layers.6.post_attention_layernorm.weight                   [LN]     rms=0.5983   âœ…
model.layers.8.post_attention_layernorm.weight                   [LN]     rms=0.5519   âœ…
model.layers.12.input_layernorm.weight                           [LN]     rms=0.4891   âœ…
model.layers.6.input_layernorm.weight                            [LN]     rms=0.5575   âœ…
model.layers.4.post_attention_layernorm.weight                   [LN]     rms=0.6595   âœ…
model.layers.5.post_attention_layernorm.weight                   [LN]     rms=0.6687   âœ…
model.norm.weight                                                [LN]     rms=0.6944   âœ…
model.layers.14.post_attention_layernorm.weight                  [LN]     rms=0.3466   âœ…
model.layers.22.input_layernorm.weight                           [LN]     rms=0.6036   âœ…
model.layers.13.input_layernorm.weight                           [LN]     rms=0.5714   âœ…
model.layers.1.post_attention_layernorm.weight                   [LN]     rms=0.8896   âœ…
model.layers.8.input_layernorm.weight                            [LN]     rms=0.5001   âœ…
model.layers.0.input_layernorm.weight                            [LN]     rms=0.5316   âœ…
model.layers.9.input_layernorm.weight                            [LN]     rms=0.5320   âœ…
model.layers.15.input_layernorm.weight                           [LN]     rms=0.5460   âœ…
model.layers.0.post_attention_layernorm.weight                   [LN]     rms=0.6287   âœ…
model.layers.20.input_layernorm.weight                           [LN]     rms=0.5601   âœ…
model.layers.22.post_attention_layernorm.weight                  [LN]     rms=0.3174   âœ…
model.layers.7.input_layernorm.weight                            [LN]     rms=0.5165   âœ…
model.layers.10.input_layernorm.weight                           [LN]     rms=0.5094   âœ…
model.layers.2.post_attention_layernorm.weight                   [LN]     rms=0.7767   âœ…
model.layers.17.post_attention_layernorm.weight                  [LN]     rms=0.3823   âœ…
model.layers.3.input_layernorm.weight                            [LN]     rms=0.3746   âœ…
model.layers.12.post_attention_layernorm.weight                  [LN]     rms=0.3454   âœ…
model.layers.15.post_attention_layernorm.weight                  [LN]     rms=0.3422   âœ…
model.layers.23.post_attention_layernorm.weight                  [LN]     rms=0.2994   âœ…
model.layers.19.post_attention_layernorm.weight                  [LN]     rms=0.3753   âœ…
model.layers.11.input_layernorm.weight                           [LN]     rms=0.5801   âœ…
model.layers.20.post_attention_layernorm.weight                  [LN]     rms=0.3987   âœ…
model.layers.2.input_layernorm.weight                            [LN]     rms=0.3511   âœ…
model.layers.13.post_attention_layernorm.weight                  [LN]     rms=0.3475   âœ…
model.layers.24.input_layernorm.weight                           [LN]     rms=0.5566   âœ…
model.layers.5.input_layernorm.weight                            [LN]     rms=0.5208   âœ…
model.layers.4.input_layernorm.weight                            [LN]     rms=0.4654   âœ…
model.layers.10.post_attention_layernorm.weight                  [LN]     rms=0.4630   âœ…
model.layers.11.post_attention_layernorm.weight                  [LN]     rms=0.4171   âœ…
model.layers.24.post_attention_layernorm.weight                  [LN]     rms=0.3322   âœ…
model.layers.25.post_attention_layernorm.weight                  [LN]     rms=0.3362   âœ…
model.layers.9.self_attn.q_proj.weight                           [PROJ]   rms=0.0409   âœ…
model.layers.23.self_attn.k_proj.weight                          [PROJ]   rms=0.0314   âœ…
model.layers.25.mlp.down_proj.weight                             [PROJ]   rms=0.0518   âœ…
model.layers.18.mlp.gate_proj.weight                             [PROJ]   rms=0.0510   âœ…
model.layers.8.mlp.up_proj.weight                                [PROJ]   rms=0.0412   âœ…
model.layers.19.self_attn.k_proj.weight                          [PROJ]   rms=0.0454   âœ…
model.layers.3.mlp.down_proj.weight                              [PROJ]   rms=0.0368   âœ…
model.layers.19.mlp.gate_proj.weight                             [PROJ]   rms=0.0506   âœ…
model.layers.20.self_attn.o_proj.weight                          [PROJ]   rms=0.0522   âœ…
model.layers.11.self_attn.k_proj.weight                          [PROJ]   rms=0.0532   âœ…
model.layers.15.mlp.up_proj.weight                               [PROJ]   rms=0.0482   âœ…
model.layers.25.mlp.up_proj.weight                               [PROJ]   rms=0.0515   âœ…
model.layers.17.mlp.up_proj.weight                               [PROJ]   rms=0.0484   âœ…
model.layers.0.mlp.down_proj.weight                              [PROJ]   rms=0.0281   âœ…
model.layers.11.self_attn.o_proj.weight                          [PROJ]   rms=0.0535   âœ…
model.layers.19.mlp.down_proj.weight                             [PROJ]   rms=0.0558   âœ…
model.layers.23.mlp.up_proj.weight                               [PROJ]   rms=0.0511   âœ…
model.layers.3.self_attn.k_proj.weight                           [PROJ]   rms=0.0197   âœ…
model.layers.13.mlp.gate_proj.weight                             [PROJ]   rms=0.0496   âœ…
model.layers.14.self_attn.q_proj.weight                          [PROJ]   rms=0.0510   âœ…
model.layers.23.self_attn.q_proj.weight                          [PROJ]   rms=0.0335   âœ…
model.layers.25.self_attn.o_proj.weight                          [PROJ]   rms=0.0269   âœ…
model.layers.24.self_attn.v_proj.weight                          [PROJ]   rms=0.0391   âœ…
model.layers.8.self_attn.q_proj.weight                           [PROJ]   rms=0.0374   âœ…
model.layers.18.self_attn.k_proj.weight                          [PROJ]   rms=0.0433   âœ…
model.layers.0.self_attn.v_proj.weight                           [PROJ]   rms=0.0357   âœ…
model.layers.12.mlp.gate_proj.weight                             [PROJ]   rms=0.0475   âœ…
model.layers.12.mlp.up_proj.weight                               [PROJ]   rms=0.0456   âœ…
model.layers.21.mlp.down_proj.weight                             [PROJ]   rms=0.0551   âœ…
model.layers.4.mlp.gate_proj.weight                              [PROJ]   rms=0.0337   âœ…
model.layers.22.self_attn.k_proj.weight                          [PROJ]   rms=0.0372   âœ…
model.layers.4.self_attn.o_proj.weight                           [PROJ]   rms=0.0320   âœ…
model.layers.8.mlp.gate_proj.weight                              [PROJ]   rms=0.0446   âœ…
model.layers.12.mlp.down_proj.weight                             [PROJ]   rms=0.0517   âœ…
model.layers.2.self_attn.v_proj.weight                           [PROJ]   rms=0.0390   âœ…
model.layers.20.self_attn.v_proj.weight                          [PROJ]   rms=0.0472   âœ…
model.layers.19.self_attn.q_proj.weight                          [PROJ]   rms=0.0470   âœ…
model.layers.21.self_attn.o_proj.weight                          [PROJ]   rms=0.0449   âœ…
model.layers.19.mlp.up_proj.weight                               [PROJ]   rms=0.0488   âœ…
model.layers.9.self_attn.v_proj.weight                           [PROJ]   rms=0.0435   âœ…
model.layers.23.mlp.down_proj.weight                             [PROJ]   rms=0.0531   âœ…
model.layers.5.self_attn.v_proj.weight                           [PROJ]   rms=0.0363   âœ…
model.layers.16.self_attn.v_proj.weight                          [PROJ]   rms=0.0539   âœ…
model.layers.10.mlp.up_proj.weight                               [PROJ]   rms=0.0432   âœ…
model.layers.10.self_attn.q_proj.weight                          [PROJ]   rms=0.0422   âœ…
model.layers.6.self_attn.o_proj.weight                           [PROJ]   rms=0.0370   âœ…
model.layers.18.mlp.up_proj.weight                               [PROJ]   rms=0.0491   âœ…
model.layers.7.self_attn.v_proj.weight                           [PROJ]   rms=0.0393   âœ…
model.layers.24.self_attn.o_proj.weight                          [PROJ]   rms=0.0390   âœ…
model.layers.11.mlp.up_proj.weight                               [PROJ]   rms=0.0449   âœ…
model.layers.14.mlp.up_proj.weight                               [PROJ]   rms=0.0483   âœ…
model.layers.24.mlp.gate_proj.weight                             [PROJ]   rms=0.0527   âœ…
model.layers.18.self_attn.q_proj.weight                          [PROJ]   rms=0.0452   âœ…
model.layers.1.self_attn.o_proj.weight                           [PROJ]   rms=0.0196   âœ…
model.layers.16.self_attn.q_proj.weight                          [PROJ]   rms=0.0563   âœ…
model.layers.2.self_attn.o_proj.weight                           [PROJ]   rms=0.0253   âœ…
model.layers.10.mlp.down_proj.weight                             [PROJ]   rms=0.0493   âœ…
model.layers.21.mlp.gate_proj.weight                             [PROJ]   rms=0.0513   âœ…
model.layers.2.self_attn.q_proj.weight                           [PROJ]   rms=0.0235   âœ…
model.layers.16.mlp.up_proj.weight                               [PROJ]   rms=0.0490   âœ…
model.layers.0.self_attn.k_proj.weight                           [PROJ]   rms=0.0267   âœ…
model.layers.6.self_attn.v_proj.weight                           [PROJ]   rms=0.0363   âœ…
model.layers.1.self_attn.q_proj.weight                           [PROJ]   rms=0.0112   âœ…
model.layers.9.self_attn.k_proj.weight                           [PROJ]   rms=0.0401   âœ…
model.layers.7.mlp.down_proj.weight                              [PROJ]   rms=0.0462   âœ…
model.layers.17.mlp.gate_proj.weight                             [PROJ]   rms=0.0508   âœ…
model.layers.20.self_attn.k_proj.weight                          [PROJ]   rms=0.0437   âœ…
model.layers.17.self_attn.o_proj.weight                          [PROJ]   rms=0.0582   âœ…
model.layers.13.self_attn.o_proj.weight                          [PROJ]   rms=0.0591   âœ…
model.layers.12.self_attn.v_proj.weight                          [PROJ]   rms=0.0504   âœ…
model.layers.20.self_attn.q_proj.weight                          [PROJ]   rms=0.0452   âœ…
model.layers.15.self_attn.o_proj.weight                          [PROJ]   rms=0.0567   âœ…
model.layers.14.mlp.gate_proj.weight                             [PROJ]   rms=0.0502   âœ…
model.layers.2.mlp.down_proj.weight                              [PROJ]   rms=0.0319   âœ…
model.layers.3.mlp.gate_proj.weight                              [PROJ]   rms=0.0304   âœ…
model.layers.1.self_attn.v_proj.weight                           [PROJ]   rms=0.0375   âœ…
model.layers.12.self_attn.o_proj.weight                          [PROJ]   rms=0.0562   âœ…
model.layers.14.self_attn.o_proj.weight                          [PROJ]   rms=0.0610   âœ…
model.layers.7.mlp.up_proj.weight                                [PROJ]   rms=0.0388   âœ…
model.layers.15.self_attn.v_proj.weight                          [PROJ]   rms=0.0527   âœ…
model.layers.1.mlp.down_proj.weight                              [PROJ]   rms=0.0276   âœ…
model.layers.19.self_attn.o_proj.weight                          [PROJ]   rms=0.0483   âœ…
model.layers.23.mlp.gate_proj.weight                             [PROJ]   rms=0.0521   âœ…
model.layers.18.mlp.down_proj.weight                             [PROJ]   rms=0.0549   âœ…
model.layers.1.self_attn.k_proj.weight                           [PROJ]   rms=0.0114   âœ…
model.layers.18.self_attn.o_proj.weight                          [PROJ]   rms=0.0534   âœ…
model.layers.9.mlp.up_proj.weight                                [PROJ]   rms=0.0429   âœ…
model.layers.9.mlp.gate_proj.weight                              [PROJ]   rms=0.0460   âœ…
model.layers.21.mlp.up_proj.weight                               [PROJ]   rms=0.0495   âœ…
model.layers.4.self_attn.q_proj.weight                           [PROJ]   rms=0.0290   âœ…
model.layers.25.self_attn.q_proj.weight                          [PROJ]   rms=0.0211   âœ…
model.layers.0.mlp.gate_proj.weight                              [PROJ]   rms=0.0258   âœ…
model.layers.15.self_attn.q_proj.weight                          [PROJ]   rms=0.0523   âœ…
model.layers.7.self_attn.k_proj.weight                           [PROJ]   rms=0.0395   âœ…
model.layers.3.self_attn.q_proj.weight                           [PROJ]   rms=0.0203   âœ…
model.layers.11.mlp.gate_proj.weight                             [PROJ]   rms=0.0476   âœ…
model.layers.13.mlp.up_proj.weight                               [PROJ]   rms=0.0473   âœ…
model.layers.24.mlp.down_proj.weight                             [PROJ]   rms=0.0525   âœ…
model.layers.21.self_attn.v_proj.weight                          [PROJ]   rms=0.0369   âœ…
model.layers.20.mlp.up_proj.weight                               [PROJ]   rms=0.0492   âœ…
model.layers.21.self_attn.k_proj.weight                          [PROJ]   rms=0.0408   âœ…
model.layers.4.self_attn.v_proj.weight                           [PROJ]   rms=0.0368   âœ…
model.layers.5.self_attn.k_proj.weight                           [PROJ]   rms=0.0333   âœ…
model.layers.16.self_attn.k_proj.weight                          [PROJ]   rms=0.0547   âœ…
model.layers.1.mlp.up_proj.weight                                [PROJ]   rms=0.0348   âœ…
model.layers.13.self_attn.q_proj.weight                          [PROJ]   rms=0.0601   âœ…
model.layers.5.self_attn.q_proj.weight                           [PROJ]   rms=0.0345   âœ…
model.layers.15.mlp.down_proj.weight                             [PROJ]   rms=0.0541   âœ…
model.layers.8.mlp.down_proj.weight                              [PROJ]   rms=0.0479   âœ…
model.layers.3.self_attn.o_proj.weight                           [PROJ]   rms=0.0300   âœ…
model.layers.12.self_attn.q_proj.weight                          [PROJ]   rms=0.0464   âœ…
model.layers.9.self_attn.o_proj.weight                           [PROJ]   rms=0.0472   âœ…
model.layers.24.self_attn.q_proj.weight                          [PROJ]   rms=0.0317   âœ…
model.layers.5.mlp.down_proj.weight                              [PROJ]   rms=0.0430   âœ…
model.layers.22.mlp.up_proj.weight                               [PROJ]   rms=0.0503   âœ…
model.layers.14.self_attn.k_proj.weight                          [PROJ]   rms=0.0500   âœ…
model.layers.11.self_attn.q_proj.weight                          [PROJ]   rms=0.0551   âœ…
model.layers.24.mlp.up_proj.weight                               [PROJ]   rms=0.0512   âœ…
model.layers.2.self_attn.k_proj.weight                           [PROJ]   rms=0.0215   âœ…
model.layers.14.self_attn.v_proj.weight                          [PROJ]   rms=0.0561   âœ…
model.layers.10.self_attn.v_proj.weight                          [PROJ]   rms=0.0457   âœ…
model.layers.16.self_attn.o_proj.weight                          [PROJ]   rms=0.0588   âœ…
model.layers.18.self_attn.v_proj.weight                          [PROJ]   rms=0.0485   âœ…
model.layers.3.mlp.up_proj.weight                                [PROJ]   rms=0.0261   âœ…
model.layers.0.self_attn.o_proj.weight                           [PROJ]   rms=0.0321   âœ…
model.layers.2.mlp.gate_proj.weight                              [PROJ]   rms=0.0278   âœ…
model.layers.8.self_attn.v_proj.weight                           [PROJ]   rms=0.0430   âœ…
model.layers.1.mlp.gate_proj.weight                              [PROJ]   rms=0.0322   âœ…
model.layers.21.self_attn.q_proj.weight                          [PROJ]   rms=0.0425   âœ…
model.layers.25.self_attn.k_proj.weight                          [PROJ]   rms=0.0198   âœ…
model.layers.8.self_attn.k_proj.weight                           [PROJ]   rms=0.0369   âœ…
model.layers.4.mlp.up_proj.weight                                [PROJ]   rms=0.0290   âœ…
model.layers.7.mlp.gate_proj.weight                              [PROJ]   rms=0.0425   âœ…
model.layers.13.mlp.down_proj.weight                             [PROJ]   rms=0.0532   âœ…
model.layers.0.self_attn.q_proj.weight                           [PROJ]   rms=0.0305   âœ…
model.layers.6.self_attn.q_proj.weight                           [PROJ]   rms=0.0385   âœ…
model.layers.11.self_attn.v_proj.weight                          [PROJ]   rms=0.0534   âœ…
model.layers.0.mlp.up_proj.weight                                [PROJ]   rms=0.0259   âœ…
model.layers.25.self_attn.v_proj.weight                          [PROJ]   rms=0.0317   âœ…
model.layers.16.mlp.gate_proj.weight                             [PROJ]   rms=0.0510   âœ…
model.layers.10.self_attn.k_proj.weight                          [PROJ]   rms=0.0413   âœ…
model.layers.15.self_attn.k_proj.weight                          [PROJ]   rms=0.0516   âœ…
model.layers.23.self_attn.v_proj.weight                          [PROJ]   rms=0.0361   âœ…
model.layers.22.mlp.down_proj.weight                             [PROJ]   rms=0.0540   âœ…
model.layers.13.self_attn.k_proj.weight                          [PROJ]   rms=0.0587   âœ…
model.layers.19.self_attn.v_proj.weight                          [PROJ]   rms=0.0444   âœ…
model.layers.22.self_attn.v_proj.weight                          [PROJ]   rms=0.0354   âœ…
model.layers.23.self_attn.o_proj.weight                          [PROJ]   rms=0.0353   âœ…
model.layers.22.self_attn.q_proj.weight                          [PROJ]   rms=0.0387   âœ…
model.layers.17.mlp.down_proj.weight                             [PROJ]   rms=0.0546   âœ…
model.layers.20.mlp.gate_proj.weight                             [PROJ]   rms=0.0512   âœ…
model.layers.22.self_attn.o_proj.weight                          [PROJ]   rms=0.0404   âœ…
model.layers.6.mlp.down_proj.weight                              [PROJ]   rms=0.0446   âœ…
model.layers.5.self_attn.o_proj.weight                           [PROJ]   rms=0.0342   âœ…
model.layers.4.self_attn.k_proj.weight                           [PROJ]   rms=0.0282   âœ…
model.layers.5.mlp.up_proj.weight                                [PROJ]   rms=0.0320   âœ…
model.layers.12.self_attn.k_proj.weight                          [PROJ]   rms=0.0460   âœ…
model.layers.20.mlp.down_proj.weight                             [PROJ]   rms=0.0559   âœ…
model.layers.7.self_attn.o_proj.weight                           [PROJ]   rms=0.0430   âœ…
model.layers.22.mlp.gate_proj.weight                             [PROJ]   rms=0.0515   âœ…
model.layers.10.mlp.gate_proj.weight                             [PROJ]   rms=0.0461   âœ…
model.layers.4.mlp.down_proj.weight                              [PROJ]   rms=0.0402   âœ…
model.layers.14.mlp.down_proj.weight                             [PROJ]   rms=0.0535   âœ…
model.layers.9.mlp.down_proj.weight                              [PROJ]   rms=0.0489   âœ…
model.layers.6.mlp.up_proj.weight                                [PROJ]   rms=0.0350   âœ…
model.layers.5.mlp.gate_proj.weight                              [PROJ]   rms=0.0370   âœ…
model.layers.24.self_attn.k_proj.weight                          [PROJ]   rms=0.0291   âœ…
model.layers.3.self_attn.v_proj.weight                           [PROJ]   rms=0.0358   âœ…
model.layers.25.mlp.gate_proj.weight                             [PROJ]   rms=0.0550   âœ…
model.layers.17.self_attn.q_proj.weight                          [PROJ]   rms=0.0527   âœ…
model.layers.2.mlp.up_proj.weight                                [PROJ]   rms=0.0255   âœ…
model.layers.16.mlp.down_proj.weight                             [PROJ]   rms=0.0537   âœ…
model.layers.11.mlp.down_proj.weight                             [PROJ]   rms=0.0513   âœ…
model.layers.13.self_attn.v_proj.weight                          [PROJ]   rms=0.0543   âœ…
model.layers.6.mlp.gate_proj.weight                              [PROJ]   rms=0.0386   âœ…
model.layers.17.self_attn.v_proj.weight                          [PROJ]   rms=0.0537   âœ…
model.layers.8.self_attn.o_proj.weight                           [PROJ]   rms=0.0446   âœ…
model.layers.17.self_attn.k_proj.weight                          [PROJ]   rms=0.0515   âœ…
model.layers.6.self_attn.k_proj.weight                           [PROJ]   rms=0.0370   âœ…
model.layers.7.self_attn.q_proj.weight                           [PROJ]   rms=0.0403   âœ…
model.layers.10.self_attn.o_proj.weight                          [PROJ]   rms=0.0517   âœ…
model.layers.15.mlp.gate_proj.weight                             [PROJ]   rms=0.0503   âœ…

âœ… LN RMS gate passed (bitnet-b1.58:f16)
âœ… Projection RMS gate passed (bitnet-b1.58:f16)
