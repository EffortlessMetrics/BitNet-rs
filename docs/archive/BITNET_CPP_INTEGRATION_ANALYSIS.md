# BitNet-rs to bitnet.cpp Integration Analysis

## Executive Summary

BitNet-rs has a **well-established FFI bridge to Microsoft's BitNet C++ implementation** with infrastructure for layer-by-layer comparison. The integration is complete for single-token logits comparison but lacks explicit APIs for intermediate activation extraction. However, the underlying llama.cpp infrastructure supports position-wise logits retrieval, enabling activation analysis through custom FFI extensions.

## 1. BITNET_CPP_DIR Environment Variable Usage

### Primary Purpose
`BITNET_CPP_DIR` points to the pre-built Microsoft BitNet C++ implementation directory.

### How It's Used

**Build-Time (bitnet-sys/build.rs):**
```
1. Locates C++ library directories: build/lib, build, lib
2. Links against libllama.so/dylib + libggml.so/dylib (if separate)
3. Sets RPATH for runtime library resolution (eliminates LD_LIBRARY_PATH)
4. Generates FFI bindings via bindgen from bitnet_c.h and llama.h
```

**Build-Time Environment Variables:**
- `BITNET_CPP_DIR` (primary)
- `BITNET_CPP_PATH` (legacy fallback)
- Default: `$HOME/.cache/bitnet_cpp` (if set)

**Build Process:**
```rust
// From crates/bitnet-sys/build.rs
let cpp_dir = env::var("BITNET_CPP_DIR")
    .or_else(|_| env::var("BITNET_CPP_PATH")) 
    .or_else(|_| env::var("HOME").map(|h| format!("{}/.cache/bitnet_cpp", h)))
    .unwrap_or_else(|_| panic!("BITNET_CPP_DIR not set"));
```

**Runtime Usage:**
- Implicit via linked libraries
- FFI calls route to C++ functions at runtime

### Setup
```bash
# Download and build C++ reference
cargo run -p xtask -- fetch-cpp

# Or manually set environment
export BITNET_CPP_DIR=$HOME/.cache/bitnet_cpp
export BITNET_CPP_PATH=/path/to/bitnet.cpp  # Legacy support
```

---

## 2. FFI Bridge Architecture

### Core Components

#### A. Header Files
**File:** `/crates/bitnet-sys/include/bitnet_c.h`

Custom C wrapper API for cross-validation:
```c
// Model lifecycle
bitnet_model_t* bitnet_model_new_from_file(const char* gguf_path);
void bitnet_model_free(bitnet_model_t*);

// Context lifecycle  
bitnet_ctx_t* bitnet_context_new(bitnet_model_t*, const bitnet_params_t*);
void bitnet_context_free(bitnet_ctx_t*);

// Core operations
int bitnet_tokenize(bitnet_model_t*, const char* text, int add_bos, int parse_special,
                    int32_t* out_ids, int out_cap);
int bitnet_eval(bitnet_ctx_t*, const int32_t* ids, int n_ids,
                float* logits_out, int logits_cap);
int bitnet_prefill(bitnet_ctx_t*, const int32_t* ids, int n_ids);

// Utilities
int bitnet_vocab_size(bitnet_ctx_t* ctx);
int bitnet_decode_greedy(bitnet_model_t* model, bitnet_ctx_t* ctx,
                         int eos_id, int eot_id, int max_steps,
                         int* out_token_ids, int out_cap);
```

#### B. C++ Shim Implementation
**File:** `/crates/bitnet-sys/csrc/bitnet_c_shim.cc`

Wrapper around llama.cpp (Microsoft's integration point):
- `bitnet_model_new_from_file()` ‚Üí `llama_load_model_from_file()`
- `bitnet_eval()` ‚Üí `llama_decode()` + `llama_get_logits()`
- `bitnet_prefill()` ‚Üí `llama_decode()` (similar but only logits for last token)
- `bitnet_decode_greedy()` ‚Üí Implements greedy token generation loop

**Key Implementation Detail (bitnet_eval):**
```cpp
int bitnet_eval(bitnet_ctx_t* c, const int32_t* ids, int n_ids,
                float* logits_out, int logits_cap) {
    // Create batch
    llama_batch batch = llama_batch_init(n_ids, 0, 1);
    
    // Populate batch tokens
    for (int i = 0; i < n_ids; i++) {
        batch.token[i] = ids[i];
        batch.pos[i] = i;
        batch.logits[i] = (i == n_ids - 1) ? 1 : 0;  // Only last token
    }
    
    // Evaluate and extract logits
    int result = llama_decode(c->context, batch);
    const float* logits = llama_get_logits(c->context);
    std::memcpy(logits_out, logits, vocab_size * sizeof(float));
    
    return 0;
}
```

#### C. Safe Rust Wrappers
**File:** `/crates/bitnet-sys/src/wrapper.rs`

Three-tier wrapper architecture:

```
Raw FFI (bindings.rs - auto-generated by bindgen)
    ‚Üì
Safe Wrappers (wrapper.rs - Model, Context, Session structs)
    ‚Üì
High-Level API (bitnet-sys lib.rs - re-exports)
```

**Key Structures:**

```rust
// Low-level C++ interaction
pub struct Model { ptr: *mut llama_model }
pub struct Context { ptr: *mut llama_context }

// Combined session for convenience
pub struct Session { 
    pub model: Model, 
    pub context: Context 
}

// Custom C shim wrappers
pub struct BitnetModel { ptr: *mut bitnet_model_t }
pub struct BitnetContext { ptr: *mut bitnet_ctx_t }
```

**Important Methods:**

```rust
// Model loading
pub fn load(path: &str) -> Result<Self>

// Tokenization  
pub fn tokenize(&self, text: &str, add_special: bool) -> Result<Vec<i32>>

// Single token batch evaluation
pub fn eval(&mut self, tokens: &[i32], n_past: i32) -> Result<()>

// Per-position logits retrieval (requires logits_all=true)
pub fn get_logits(&self) -> Result<Vec<f32>>
pub fn get_logits_ith(&self, i: i32) -> Result<Vec<f32>>
pub fn get_all_logits(&self, n_tokens: usize) -> Result<Vec<Vec<f32>>>

// Generation
pub fn generate_greedy(&mut self, prompt: &str, max_tokens: usize) -> Result<Vec<i32>>
```

---

## 3. Cross-Validation Tests

### Test Infrastructure Location
- **Main Suite:** `/crossval/` crate
- **Parity Tests:** `/crossval/tests/parity_bitnetcpp.rs`
- **Comparison Module:** `/crossval/src/comparison.rs`

### Key Cross-Validation Tests

#### A. Parity Bitnetcpp Test Suite
**File:** `/crossval/tests/parity_bitnetcpp.rs`

Tests Rust implementation against C++ reference with deterministic execution:
- Single-step logits comparison (tolerance: 1e-4)
- Multi-step token sequence comparison
- Greedy sampling validation
- Cosine similarity metrics
- JSON receipt generation

**Execution:**
```bash
export CROSSVAL_GGUF=/path/to/model.gguf
export BITNET_CPP_DIR=$HOME/.cache/bitnet_cpp

# Run parity tests
cargo test -p crossval --features crossval,integration-tests \
  parity_bitnetcpp -- --nocapture
```

**Receipt Output:** `docs/baselines/YYYY-MM-DD/parity-bitnetcpp.json`
- Model SHA256 fingerprint
- Tokenization metadata
- Rust logits and tokens
- C++ parity metrics (cosine similarity, exact match rate)

#### B. General Parity Tests
**File:** `/crossval/tests/parity.rs`

Framework for deterministic comparison:
- Helper functions for comparing logits with tolerance
- Top-5 token debugging output
- Per-step validation logic

#### C. FFI Integration Tests
**File:** `/crossval/tests/ffi_integration.rs`

Low-level FFI binding tests:
- Model loading via C++ FFI
- Tokenization via C++ FFI
- Logits extraction

### Available APIs for Cross-Validation

**Rust-side evaluation:**
```rust
use bitnet_inference::eval_logits_once;
use bitnet_sys::wrapper::Session as CppSession;

// Rust evaluation
let rust_logits = eval_logits_once(model_path, &tokens)?;

// C++ evaluation (via FFI)
let mut cpp_session = CppSession::load_deterministic(model_path)?;
let cpp_logits = cpp_session.eval_and_get_logits(&tokens, 0)?;
```

---

## 4. Build Integration

### Build Scripts

#### bitnet-sys/build.rs
**Responsibility:** Link C++ FFI and generate Rust bindings

**Steps:**
1. Check if `ffi` feature is enabled
2. Locate BITNET_CPP_DIR
3. Set library search paths with RPATH
4. Link against libllama + libggml + platform C++ stdlib
5. Compile bitnet_c_shim.cc via xtask_build_helper
6. Generate bindings.rs via bindgen

**Handles:**
- Missing C++ implementation with clear error messages
- Platform-specific linking (Linux/macOS/Windows)
- RPATH setup for runtime resolution

#### crossval/build.rs
**Responsibility:** Optional C++ library discovery

**Steps:**
1. Compile bitnet_cpp_wrapper.c
2. Search for C++ libraries (more flexible)
3. Set `cfg(have_cpp)` flag if libraries found
4. Link with stdc++ (Linux) or libc++ (macOS)

**Falls back gracefully:**
- Uses mock C wrapper if library directories not found
- Prints warnings but doesn't fail the build

### Feature Flags

```rust
// In Cargo.toml
[features]
ffi = ["bindgen", "cc"]  # Enable FFI binding generation
crossval = ["ffi"]        # Backward compatibility alias

[dev-dependencies]
bitnet-sys = { features = ["ffi"] }
```

**Usage:**
```bash
# Build with FFI support
cargo build --features ffi

# Build with cross-validation
cargo test --features crossval

# Build without FFI (pure Rust)
cargo build --no-default-features --features cpu
```

---

## 5. Tokenizer Parity

### Current Status
- **Tokenization:** Parity tests verify Rust and C++ tokenizers produce identical tokens
- **Blocking Issue:** #469 (Tokenizer parity + FFI build hygiene)
- **Mitigation:** Using C++ tokenizer for both sides in parity tests

### Existing Tokenizer Comparison

**File:** `/crossval/tests/token_equivalence.rs`

Compares tokenization outputs:
```rust
// Test that Rust and C++ tokenizers match
fn test_tokenization_parity() {
    let cpp_tokens = cpp_session.tokenize(prompt)?;
    let rust_tokens = rust_tokenizer.encode(prompt)?;
    assert_eq!(cpp_tokens, rust_tokens);
}
```

---

## 6. Available bitnet.cpp APIs

### Direct C++ Access (via FFI)

**From llama.cpp (exposed via bindings.rs):**
```rust
// Model loading
pub unsafe fn llama_load_model_from_file(path: *const c_char, params: llama_model_params) -> *mut llama_model
pub unsafe fn llama_free_model(model: *mut llama_model)

// Context creation
pub unsafe fn llama_new_context_with_model(model: *mut llama_model, params: llama_context_params) -> *mut llama_context
pub unsafe fn llama_free(ctx: *mut llama_context)

// Evaluation
pub unsafe fn llama_decode(ctx: *mut llama_context, batch: llama_batch) -> c_int
pub unsafe fn llama_batch_init(n_tokens: c_int, embd: c_int, n_seq_max: c_int) -> llama_batch
pub unsafe fn llama_batch_free(batch: llama_batch)

// Logits extraction (KEY FOR LAYER ANALYSIS)
pub unsafe fn llama_get_logits(ctx: *mut llama_context) -> *const c_float
pub unsafe fn llama_get_logits_ith(ctx: *mut llama_context, i: c_int) -> *const c_float

// Tokenization
pub unsafe fn llama_tokenize(model: *mut llama_model, text: *const c_char, text_len: c_int,
                              tokens: *mut llama_token, n_max: c_int, add_special: bool, parse_special: bool) -> c_int

// Utilities
pub unsafe fn llama_n_vocab(model: *mut llama_model) -> c_int
pub unsafe fn llama_get_kv_cache_token_count(ctx: *mut llama_context) -> i32
```

**From Custom C Shim (bitnet_c_shim.cc):**
```rust
pub fn bitnet_tokenize_text(model: &BitnetModel, text: &str, add_bos: bool, parse_special: bool) -> Result<Vec<i32>>
pub fn bitnet_eval_tokens(ctx: &BitnetContext, ids: &[i32], vocab_size: usize) -> Result<Vec<f32>>
pub fn bitnet_prefill(ctx: &BitnetContext, ids: &[i32]) -> Result<()>
pub fn cpp_vocab_size(ctx: &BitnetContext) -> Result<usize>
pub fn cpp_decode_greedy(ctx: &BitnetContext, eos_id: u32, eot_id: Option<u32>, max_steps: usize) -> Result<Vec<u32>>
```

---

## 7. Can We Extract Intermediate Activations?

### Current Capability: NO (Direct Layer Output)
- **llama.cpp limitation:** No exposed API for layer-wise hidden states
- **Alternative:** Use logits as proxy for final layer analysis

### Potential Workarounds

#### Option 1: Logits-Based Analysis (Feasible Now)
Use `llama_get_logits_ith()` to extract logits at each token position:
```rust
// Setup: require logits_all=true in context creation
let mut batch = llama_batch_init(n_tokens, 0, 1);
for i in 0..n_tokens {
    batch.logits[i] = 1;  // Request logits for ALL positions
}
llama_decode(ctx, batch);

// Extract per-position logits
for pos in 0..n_tokens {
    let logits = llama_get_logits_ith(ctx, pos as i32)?;
    // Compare Rust vs C++ logits at each position
}
```

**What this gives us:**
- Per-token logits from final layer
- Position-dependent comparison
- Can detect where Rust/C++ diverge in generation

#### Option 2: Extend C Shim for Layer Extraction
Add new FFI functions to bitnet_c_shim.cc:

```cpp
// NEW: Extract activations from specific layer
float* bitnet_get_layer_output(bitnet_ctx_t* c, int layer_idx);
int bitnet_get_all_layer_outputs(bitnet_ctx_t* c, int layer_idx, float* out, int cap);

// NEW: Evaluate with intermediate capture
int bitnet_eval_with_intermediates(bitnet_ctx_t* c, const int32_t* ids, int n_ids,
                                    layer_outputs_t* out);
```

**Challenge:** Requires modifying llama.cpp to expose intermediate activations
- Not currently exposed in standard llama.cpp API
- Would need to fork or patch llama.cpp

#### Option 3: C++ Reference Implementation Protocol Buffer
Create separate C++ debug binary that:
1. Loads same model
2. Evaluates same tokens
3. Dumps layer-by-layer activations to JSON/protobuf
4. Rust test suite loads and compares

**Less intrusive:** Doesn't require FFI modification

---

## 8. Existing Infrastructure for Parallel Execution

### Session-Based Comparison
**File:** `/crates/bitnet-inference/src/ffi_session.rs`

```rust
/// Global FFI session for parity checking (reused across tests)
static PARITY_CPP_SESSION: OnceCell<Mutex<ParityCppSession>> = OnceCell::new();

pub struct ParityCppSession {
    model_path: String,
    vocab_size: usize,
    context: BitnetContext,
    model: BitnetModel,
}

impl ParityCppSession {
    pub fn new(model_path: &str) -> Result<Self> { ... }
    pub fn ensure_model(&mut self, model_path: &str) -> Result<()> { ... }
    pub fn prefill(&self, tokens: &[i32]) -> Result<()> { ... }
    pub fn eval_last_logits(&self, tokens: &[i32]) -> Result<Vec<f32>> { ... }
}
```

**Allows:**
- Reusable session across multiple test runs
- Prevents repeated model/context allocation (which caused crashes)
- Single-threaded access via Mutex ensures FFI safety

---

## Summary: What Gaps Exist for Layer-by-Layer Comparison?

### What We Have ‚úÖ
1. **Logits-level comparison** - Fully functional and tested
2. **Token generation** - Greedy decoding validated
3. **Tokenization** - Parity tests in place
4. **Cross-validation framework** - Complete infrastructure
5. **FFI bridge to C++** - Stable and well-integrated

### What We DON'T Have ‚ùå
1. **Intermediate activation extraction** - Not exposed in llama.cpp
2. **Layer-wise hidden state comparison** - No FFI API
3. **Attention map extraction** - Not available in llama.cpp wrapper
4. **Position-wise layer analysis** - Would need custom instrumentation

### What We COULD Add üîß
1. **Per-position logits analysis** - Use `llama_get_logits_ith()` 
2. **Custom C shim extensions** - Patch bitnet_c_shim.cc to expose more
3. **Instrumentation layer** - Fork llama.cpp to add debug output
4. **Reference implementations** - Separate C++ dump tool

---

## Recommended Next Steps

### For Logits-Level Debugging (Quick Win)
1. Enhance `bitnet_eval()` to optionally return per-position logits
2. Add `bitnet_get_logits_ith()` wrapper to C shim
3. Create Rust test that compares position-wise divergence

### For Full Layer Analysis (Longer Term)
1. Evaluate need for actual intermediate activations
2. Consider forking llama.cpp or adding debug APIs
3. Or: Create separate C++ diagnostic tool

### For Immediate Production Validation
- Current logits comparison is sufficient for correctness validation
- Token-level parity tests catch semantic divergence
- Cosine similarity metrics provide confidence bounds
