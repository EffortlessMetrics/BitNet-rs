# v0.2.0 Release Notes

**Highlight: Intel GPU Support & Multi-Backend Architecture**

v0.2.0 is the largest release of bitnet-rs to date, bringing first-class Intel
Arc GPU support and a multi-backend kernel dispatch architecture that opens the
door to Vulkan, ROCm, Metal, and WebGPU acceleration.

## By the Numbers

- **140+ PRs** merged since v0.1.0
- **7 GPU microcrates**: `bitnet-opencl`, `bitnet-vulkan`, `bitnet-webgpu`,
  `bitnet-metal`, `bitnet-rocm`, `bitnet-level-zero`, `bitnet-gpu-hal`
- **25+ OpenCL kernels** for Intel Arc GPUs
- **6 WGSL shaders** (WebGPU), **6 MSL shaders** (Metal), **6 HIP kernels** (ROCm)
- **1 500+ new tests** across the workspace
- **6 Architecture Decision Records** documenting major design choices
- **15+ new documentation pages** covering GPU setup, backend selection,
  telemetry, and migration

## Key Features

### Intel Arc GPU Inference

Run 1-bit LLM inference on Intel Arc GPUs via the OpenCL backend. Enable with
`--features opencl` and select the device with `--device opencl:0`.

### Multi-Backend Kernel Dispatch

`KernelManager` now supports runtime selection across CUDA, OpenCL, Vulkan, and
CPU backends. The best available backend is chosen automatically, or you can pin
a specific one with the `--device` flag.

### GPU Hardware Abstraction Layer (`bitnet-gpu-hal`)

A thin HAL unifies buffer management, kernel launch, and synchronization across
all GPU backends, simplifying future backend additions.

### GPU Telemetry & Metrics

Per-kernel timing, memory utilization, and queue-depth metrics are available via
`BITNET_GPU_TELEMETRY=1`. Results integrate with the existing receipt system.

### Paged KV Cache

Paged memory allocation for the KV cache reduces fragmentation and improves
throughput for long-context inference workloads.

### GPU-Accelerated Sampling

Top-k, top-p, and temperature sampling execute on-device when a GPU backend is
active, eliminating host round-trips.

## CI & Infrastructure

- **Nightly fuzz workflow**: 15 targets × 60 s with corpus caching
- **Weekly GPU smoke lane**: CUDA smoke tests with receipt artifact upload
- **Docker CI**: `docker-compose.test.yml` for hermetic GPU testing
- **Feature matrix CI**: automated cross-product of backends × platforms

## Breaking Changes

See the [Migration Guide](MIGRATION_GUIDE_v0.2.0.md) for details:

- `Device` enum expanded with `OpenCL`, `Vulkan`, `Rocm`, `Metal`, `WebGpu`
  variants
- New feature flags: `oneapi`, `opencl`, `vulkan`, `webgpu`, `rocm`, `metal`
- `KernelBackend` expanded with corresponding variants
- `BackendStartupSummary` includes GPU backend information

## Known Limitations

- **QK256 Performance**: Scalar kernels remain at ~0.1 tok/s for 2B models.
  AVX2 nibble-LUT + FMA tiling work has started but is not complete in this
  release. Use I2_S BitNet32-F16 format for faster CPU inference.
- **OpenCL backend**: Tested primarily on Intel Arc A-series. Other OpenCL
  devices may require driver-specific tuning.
- **WebGPU/Metal/ROCm backends**: Functional with shader coverage but not yet
  production-hardened. Treat as experimental.

## Acknowledgements

Thanks to all contributors who made this release possible. Special thanks to the
Intel GPU enablement team and the community members who tested early builds on
Arc hardware.
