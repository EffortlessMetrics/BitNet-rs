#!/bin/bash

# BitNet-rs Test Template Generator
# This script helps developers quickly create test files from templates

set -e

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# Function to print colored output
print_color() {
    printf "${1}${2}${NC}\n"
}

print_header() {
    echo
    print_color $BLUE "ðŸ§ª BitNet-rs Test Template Generator"
    print_color $BLUE "===================================="
    echo
}

print_usage() {
    echo "Usage: $0 [OPTIONS] <test-name>"
    echo
    echo "Options:"
    echo "  -t, --type TYPE     Test type: unit, integration, performance (default: unit)"
    echo "  -m, --module NAME   Module name for unit tests (e.g., bitnet_common)"
    echo "  -h, --help          Show this help message"
    echo
    echo "Examples:"
    echo "  $0 my_feature_test                    # Create unit test"
    echo "  $0 -t integration workflow_test       # Create integration test"
    echo "  $0 -t unit -m bitnet_models model_test # Create unit test in specific module"
    echo "  $0 -t performance benchmark_test      # Create performance test"
}

# Default values
TEST_TYPE="unit"
MODULE_NAME=""
TEST_NAME=""

# Parse command line arguments
while [[ $# -gt 0 ]]; do
    case $1 in
        -t|--type)
            TEST_TYPE="$2"
            shift 2
            ;;
        -m|--module)
            MODULE_NAME="$2"
            shift 2
            ;;
        -h|--help)
            print_header
            print_usage
            exit 0
            ;;
        -*)
            print_color $RED "Error: Unknown option $1"
            print_usage
            exit 1
            ;;
        *)
            if [[ -z "$TEST_NAME" ]]; then
                TEST_NAME="$1"
            else
                print_color $RED "Error: Multiple test names provided"
                print_usage
                exit 1
            fi
            shift
            ;;
    esac
done

# Validate inputs
if [[ -z "$TEST_NAME" ]]; then
    print_color $RED "Error: Test name is required"
    print_usage
    exit 1
fi

if [[ ! "$TEST_TYPE" =~ ^(unit|integration|performance)$ ]]; then
    print_color $RED "Error: Test type must be one of: unit, integration, performance"
    exit 1
fi

print_header

# Determine target directory and file path
case $TEST_TYPE in
    unit)
        if [[ -n "$MODULE_NAME" ]]; then
            TARGET_DIR="tests/unit/$MODULE_NAME"
            TARGET_FILE="$TARGET_DIR/test_$TEST_NAME.rs"
        else
            TARGET_DIR="tests/unit"
            TARGET_FILE="$TARGET_DIR/test_$TEST_NAME.rs"
        fi
        ;;
    integration)
        TARGET_DIR="tests/integration"
        TARGET_FILE="$TARGET_DIR/${TEST_NAME}_integration_test.rs"
        ;;
    performance)
        TARGET_DIR="tests/performance"
        TARGET_FILE="$TARGET_DIR/${TEST_NAME}_performance_test.rs"
        ;;
esac

# Create target directory if it doesn't exist
mkdir -p "$TARGET_DIR"

# Check if file already exists
if [[ -f "$TARGET_FILE" ]]; then
    print_color $YELLOW "Warning: File $TARGET_FILE already exists"
    read -p "Do you want to overwrite it? (y/N): " -n 1 -r
    echo
    if [[ ! $REPLY =~ ^[Yy]$ ]]; then
        print_color $BLUE "Operation cancelled"
        exit 0
    fi
fi

# Generate test content based on type
generate_unit_test() {
    cat > "$TARGET_FILE" << EOF
//! Unit tests for $TEST_NAME
//!
//! This file contains unit tests for the $TEST_NAME functionality.
//! Generated by BitNet-rs test template generator.

use bitnet_tests::prelude::*;
use std::time::Duration;

/// Example function to test
pub fn example_function(input: &str) -> Result<String, String> {
    if input.is_empty() {
        return Err("Input cannot be empty".to_string());
    }

    Ok(format!("Processed: {}", input))
}

#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_${TEST_NAME}_success() {
        // Arrange
        let input = "test input";

        // Act
        let result = example_function(input);

        // Assert
        assert!(result.is_ok());
        assert_eq!(result.unwrap(), "Processed: test input");
    }

    #[tokio::test]
    async fn test_${TEST_NAME}_empty_input() {
        // Arrange
        let input = "";

        // Act
        let result = example_function(input);

        // Assert
        assert!(result.is_err());
        assert_eq!(result.unwrap_err(), "Input cannot be empty");
    }

    #[tokio::test]
    async fn test_${TEST_NAME}_multiple_cases() {
        let test_cases = vec![
            ("hello", "Processed: hello"),
            ("world", "Processed: world"),
            ("test123", "Processed: test123"),
        ];

        for (input, expected) in test_cases {
            let result = example_function(input).unwrap();
            assert_eq!(result, expected, "Failed for input: {}", input);
        }
    }

    // TODO: Add more specific tests for your functionality
    // - Error conditions
    // - Edge cases
    // - Performance requirements
    // - Resource management
}

// Run with: cargo test --test test_$TEST_NAME
EOF
}

generate_integration_test() {
    cat > "$TARGET_FILE" << EOF
//! Integration tests for $TEST_NAME
//!
//! This file contains integration tests that validate complete workflows
//! and component interactions for $TEST_NAME.
//! Generated by BitNet-rs test template generator.

use bitnet_tests::common::{TestUtilities, TestError};
use std::path::PathBuf;
use std::time::Duration;
use tempfile::TempDir;

/// Example system for integration testing
pub struct TestSystem {
    config: SystemConfig,
}

#[derive(Debug, Clone)]
pub struct SystemConfig {
    pub name: String,
    pub enabled: bool,
}

impl TestSystem {
    pub async fn new(config: SystemConfig) -> Result<Self, TestError> {
        if config.name.is_empty() {
            return Err(TestError::setup("System name cannot be empty"));
        }

        Ok(Self { config })
    }

    pub async fn process_workflow(&self, input: &str) -> Result<String, TestError> {
        if !self.config.enabled {
            return Err(TestError::execution("System is disabled"));
        }

        // Simulate multi-step workflow
        let step1_result = self.step1_process(input).await?;
        let step2_result = self.step2_transform(&step1_result).await?;
        let final_result = self.step3_finalize(&step2_result).await?;

        Ok(final_result)
    }

    async fn step1_process(&self, input: &str) -> Result<String, TestError> {
        tokio::time::sleep(Duration::from_millis(10)).await;
        Ok(format!("Step1: {}", input))
    }

    async fn step2_transform(&self, input: &str) -> Result<String, TestError> {
        tokio::time::sleep(Duration::from_millis(10)).await;
        Ok(format!("Step2: {}", input.to_uppercase()))
    }

    async fn step3_finalize(&self, input: &str) -> Result<String, TestError> {
        tokio::time::sleep(Duration::from_millis(10)).await;
        Ok(format!("Final: {}", input))
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    fn create_test_config() -> SystemConfig {
        SystemConfig {
            name: "test_system".to_string(),
            enabled: true,
        }
    }

    #[tokio::test]
    async fn test_${TEST_NAME}_complete_workflow() {
        // Setup
        let config = create_test_config();
        let system = TestSystem::new(config).await.unwrap();

        // Execute complete workflow
        let result = system.process_workflow("test input").await;

        // Verify
        assert!(result.is_ok());
        let output = result.unwrap();
        assert!(output.contains("STEP1: TEST INPUT"));
        assert!(output.starts_with("Final: Step2: Step1:"));
    }

    #[tokio::test]
    async fn test_${TEST_NAME}_with_files() {
        // Setup temporary directory
        let temp_dir = TempDir::new().unwrap();
        let input_file = temp_dir.path().join("input.txt");
        let output_file = temp_dir.path().join("output.txt");

        // Create test input
        let test_data = "integration test data";
        TestUtilities::write_test_file(&input_file, test_data.as_bytes()).await.unwrap();

        // Process through system
        let config = create_test_config();
        let system = TestSystem::new(config).await.unwrap();

        let input_content = TestUtilities::read_test_file(&input_file).await.unwrap();
        let input_str = String::from_utf8(input_content).unwrap();

        let result = system.process_workflow(&input_str).await.unwrap();

        // Write output
        TestUtilities::write_test_file(&output_file, result.as_bytes()).await.unwrap();

        // Verify output file
        assert!(output_file.exists());
        let output_content = TestUtilities::read_test_file(&output_file).await.unwrap();
        let output_str = String::from_utf8(output_content).unwrap();

        assert!(output_str.contains("INTEGRATION TEST DATA"));
    }

    #[tokio::test]
    async fn test_${TEST_NAME}_error_handling() {
        // Test with disabled system
        let disabled_config = SystemConfig {
            name: "test_system".to_string(),
            enabled: false,
        };
        let system = TestSystem::new(disabled_config).await.unwrap();

        let result = system.process_workflow("test").await;
        assert!(result.is_err());
    }

    #[tokio::test]
    async fn test_${TEST_NAME}_performance() {
        let config = create_test_config();
        let system = TestSystem::new(config).await.unwrap();

        let start = std::time::Instant::now();
        let result = system.process_workflow("performance test").await;
        let duration = start.elapsed();

        assert!(result.is_ok());
        assert!(duration < Duration::from_millis(100),
               "Workflow should complete within 100ms, took {:?}", duration);
    }

    // TODO: Add more integration tests
    // - Component interaction tests
    // - Configuration validation
    // - Resource management
    // - Concurrent access
}

// Run with: cargo test --test ${TEST_NAME}_integration_test
EOF
}

generate_performance_test() {
    cat > "$TARGET_FILE" << EOF
//! Performance tests for $TEST_NAME
//!
//! This file contains performance benchmarks and validation tests
//! for $TEST_NAME functionality.
//! Generated by BitNet-rs test template generator.

use bitnet_tests::common::{TestUtilities, TestError};
use std::time::{Duration, Instant};

/// Example function for performance testing
pub async fn performance_function(data: &[u8]) -> Result<Vec<u8>, TestError> {
    // Simulate processing
    tokio::time::sleep(Duration::from_micros(data.len() as u64)).await;

    let mut result = Vec::with_capacity(data.len());
    for &byte in data {
        result.push(byte.wrapping_add(1));
    }

    Ok(result)
}

#[cfg(test)]
mod tests {
    use super::*;

    #[tokio::test]
    async fn test_${TEST_NAME}_performance_small_data() {
        let test_data = vec![0u8; 1024]; // 1KB

        let start = Instant::now();
        let result = performance_function(&test_data).await;
        let duration = start.elapsed();

        assert!(result.is_ok());
        assert!(duration < Duration::from_millis(10),
               "Small data processing should be fast, took {:?}", duration);
    }

    #[tokio::test]
    async fn test_${TEST_NAME}_performance_large_data() {
        let test_data = vec![0u8; 1024 * 1024]; // 1MB

        let start = Instant::now();
        let result = performance_function(&test_data).await;
        let duration = start.elapsed();

        assert!(result.is_ok());
        assert!(duration < Duration::from_secs(1),
               "Large data processing should complete within 1s, took {:?}", duration);
    }

    #[tokio::test]
    async fn test_${TEST_NAME}_memory_usage() {
        let test_data = vec![0u8; 10 * 1024 * 1024]; // 10MB

        let memory_before = TestUtilities::get_memory_usage();
        let result = performance_function(&test_data).await;
        let memory_after = TestUtilities::get_memory_usage();

        assert!(result.is_ok());

        let memory_increase = memory_after.saturating_sub(memory_before);
        assert!(memory_increase < 50 * 1024 * 1024,
               "Memory usage should be reasonable, increased by {} bytes", memory_increase);
    }

    #[tokio::test]
    async fn test_${TEST_NAME}_throughput() {
        let iterations = 100;
        let test_data = vec![0u8; 1024];
        let mut durations = Vec::new();

        // Warm up
        for _ in 0..10 {
            let _ = performance_function(&test_data).await;
        }

        // Measure throughput
        for _ in 0..iterations {
            let start = Instant::now();
            let result = performance_function(&test_data).await;
            let duration = start.elapsed();

            assert!(result.is_ok());
            durations.push(duration);
        }

        // Calculate statistics
        let total_time: Duration = durations.iter().sum();
        let avg_time = total_time / iterations;
        let throughput = 1.0 / avg_time.as_secs_f64(); // operations per second

        println!("Average time per operation: {:?}", avg_time);
        println!("Throughput: {:.2} ops/sec", throughput);

        assert!(throughput > 100.0, "Throughput should be at least 100 ops/sec, got {:.2}", throughput);
    }

    #[tokio::test]
    async fn test_${TEST_NAME}_scalability() {
        let data_sizes = vec![1024, 10240, 102400, 1024000]; // 1KB to 1MB
        let mut results = Vec::new();

        for size in data_sizes {
            let test_data = vec![0u8; size];

            let start = Instant::now();
            let result = performance_function(&test_data).await;
            let duration = start.elapsed();

            assert!(result.is_ok());

            let throughput = size as f64 / duration.as_secs_f64(); // bytes per second
            results.push((size, duration, throughput));

            println!("Size: {} bytes, Time: {:?}, Throughput: {:.2} bytes/sec",
                    size, duration, throughput);
        }

        // Verify scalability - throughput should remain relatively stable
        let throughputs: Vec<f64> = results.iter().map(|(_, _, t)| *t).collect();
        let min_throughput = throughputs.iter().fold(f64::INFINITY, |a, &b| a.min(b));
        let max_throughput = throughputs.iter().fold(0.0, |a, &b| a.max(b));
        let throughput_ratio = max_throughput / min_throughput;

        assert!(throughput_ratio < 10.0,
               "Throughput should scale reasonably, ratio: {:.2}", throughput_ratio);
    }

    #[tokio::test]
    async fn test_${TEST_NAME}_concurrent_performance() {
        let concurrent_tasks = 10;
        let test_data = vec![0u8; 10240]; // 10KB per task

        let start = Instant::now();

        let mut handles = Vec::new();
        for _ in 0..concurrent_tasks {
            let data = test_data.clone();
            let handle = tokio::spawn(async move {
                performance_function(&data).await
            });
            handles.push(handle);
        }

        let mut results = Vec::new();
        for handle in handles {
            let result = handle.await.unwrap();
            assert!(result.is_ok());
            results.push(result.unwrap());
        }

        let total_duration = start.elapsed();

        assert_eq!(results.len(), concurrent_tasks);
        assert!(total_duration < Duration::from_secs(1),
               "Concurrent processing should be efficient, took {:?}", total_duration);
    }

    // TODO: Add more performance tests
    // - Stress testing
    // - Resource exhaustion scenarios
    // - Performance regression detection
    // - Comparison with baseline implementations
}

// Run with: cargo test --test ${TEST_NAME}_performance_test
EOF
}

# Generate the appropriate test file
print_color $BLUE "Creating $TEST_TYPE test: $TEST_NAME"
print_color $BLUE "Target file: $TARGET_FILE"

case $TEST_TYPE in
    unit)
        generate_unit_test
        ;;
    integration)
        generate_integration_test
        ;;
    performance)
        generate_performance_test
        ;;
esac

print_color $GREEN "âœ… Test file created successfully!"
echo

print_color $YELLOW "Next steps:"
echo "1. Edit the generated test file to implement your specific test logic"
echo "2. Replace the example functions with your actual code under test"
echo "3. Add more test cases as needed"
echo "4. Run your test with:"
print_color $BLUE "   cargo test --test $(basename "$TARGET_FILE" .rs)"
echo

print_color $YELLOW "Useful resources:"
echo "â€¢ Quick Start Guide: docs/testing/quick-start-guide.md"
echo "â€¢ Test Templates: docs/testing/test-templates.md"
echo "â€¢ Test Authoring Guide: docs/testing/test-authoring-guide.md"
echo "â€¢ Example tests: tests/examples/"
echo

print_color $GREEN "Happy testing! ðŸ§ª"
