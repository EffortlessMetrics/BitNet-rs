#!/usr/bin/env python3
"""
TDD Receipt Generator for BitNet.rs

Generates test status receipts from actual cargo nextest runs and updates
README.md and CLAUDE.md with concrete, verifiable test counts.

Usage:
    python3 scripts/tdd_receipts.py
"""
import pathlib
import re
import subprocess
import json
import datetime

ROOT = pathlib.Path(__file__).resolve().parents[1]
OUT = ROOT / "docs/tdd/receipts"
OUT.mkdir(parents=True, exist_ok=True)

def sh(cmd):
    """Run shell command and return result"""
    return subprocess.run(cmd, shell=True, text=True, capture_output=True, check=False)

def list_tests():
    """Count total discoverable tests"""
    # Try nextest first
    p = sh("cargo nextest list --all --no-default-features --features cpu --format terse 2>&1")
    if p.returncode == 0:
        total = len([l for l in p.stdout.splitlines() if l.strip() and not l.startswith("    ") and "::" in l])
        return total

    # Fall back to cargo test --list
    print("  nextest not available, using cargo test --list...")
    p = sh("cargo test --workspace --no-default-features --features cpu --lib --bins --tests -- --list 2>&1")
    if p.returncode != 0:
        print(f"Warning: cargo test --list failed with code {p.returncode}")
        return 0

    # Parse test names from cargo test --list output
    # Format: "test_name: test"
    total = len([l for l in p.stdout.splitlines() if ": test" in l])
    return total

def run_cpu():
    """Run CPU tests and parse results"""
    tail_file = OUT / "nextest_cpu_tail.txt"

    # Try nextest first, fall back to cargo test
    nextest_check = sh("cargo nextest --version 2>&1")
    if nextest_check.returncode == 0:
        cmd = (
            "PROPTEST_CASES=128 cargo nextest run --all --no-default-features --features cpu "
            "--retries=0 --failure-output=final --status-level=final 2>&1"
        )
    else:
        print("  nextest not available, using cargo test...")
        cmd = (
            "PROPTEST_CASES=128 cargo test --workspace --no-default-features --features cpu "
            "--lib --bins --tests 2>&1"
        )

    p = sh(cmd)

    # Save tail for debugging
    tail_file.write_text("\n".join(p.stdout.splitlines()[-500:]))

    # Parse summary lines
    passed = failed = ignored = 0
    for line in p.stdout.splitlines():
        if "test result:" in line.lower() or "summary" in line.lower():
            # Try to extract numbers from various formats
            # Example: "test result: ok. 19 passed; 0 failed; 0 ignored;"
            parts = re.findall(r'(\d+)\s+passed', line)
            if parts:
                passed += int(parts[0])
            parts = re.findall(r'(\d+)\s+failed', line)
            if parts:
                failed += int(parts[0])
            parts = re.findall(r'(\d+)\s+ignored', line)
            if parts:
                ignored += int(parts[0])

    return {"passed": passed, "failed": failed, "ignored": ignored}

def summarize_skips():
    """Extract skip reasons from test output"""
    path = OUT / "nextest_cpu_tail.txt"
    if not path.exists():
        return {}

    counts = {}
    for line in path.read_text().splitlines():
        if "skip:" in line.lower():
            # Normalize skip reason
            skip_line = line.strip()
            counts[skip_line] = counts.get(skip_line, 0) + 1

    return counts

def render_status(total, run, skips):
    """Render test status as markdown"""
    ts = datetime.datetime.utcnow().isoformat(timespec="seconds") + "Z"
    lines = []
    lines.append(f"**As of {ts} (CPU features)**")
    lines.append(f"")
    lines.append(f"- **Discovered**: {total} tests")
    lines.append(f"- **Executed**: {run['passed'] + run['failed'] + run['ignored']} tests")
    lines.append(f"  - Passed: {run['passed']}")
    lines.append(f"  - Failed: {run['failed']}")
    lines.append(f"  - Ignored: {run['ignored']}")

    if skips:
        lines.append(f"")
        lines.append(f"**Top skip reasons:**")
        for k, v in sorted(skips.items(), key=lambda kv: -kv[1])[:6]:
            lines.append(f"- {k} (×{v})")

    lines.append("")
    lines.append("_Auto-generated by `scripts/tdd_receipts.py`. Run `just tdd-receipts` to refresh._")
    return "\n".join(lines)

def update_markers(path, payload):
    """Update TEST-STATUS markers in a file"""
    if not path.exists():
        print(f"Warning: {path} does not exist, skipping")
        return

    text = path.read_text()
    new = re.sub(
        r"<!-- TEST-STATUS:BEGIN -->.*?<!-- TEST-STATUS:END -->",
        f"<!-- TEST-STATUS:BEGIN -->\n{payload}\n<!-- TEST-STATUS:END -->",
        text,
        flags=re.S
    )

    if new != text:
        path.write_text(new)
        print(f"Updated {path.name}")
    else:
        print(f"No changes needed for {path.name}")

def main():
    """Main entry point"""
    print("Generating TDD receipts...")

    # Gather test data
    print("1/4 Listing tests...")
    total = list_tests()

    print("2/4 Running CPU tests...")
    run = run_cpu()

    print("3/4 Analyzing skip reasons...")
    skips = summarize_skips()

    print("4/4 Updating documentation...")
    payload = render_status(total, run, skips)

    # Update both files
    for fname in ["README.md", "CLAUDE.md"]:
        p = ROOT / fname
        update_markers(p, payload)

    # Save JSON receipt
    receipt = {
        "total": total,
        "passed": run["passed"],
        "failed": run["failed"],
        "ignored": run["ignored"],
        "skips": skips,
        "timestamp": datetime.datetime.utcnow().isoformat() + "Z"
    }

    receipt_path = OUT / "status.json"
    receipt_path.write_text(json.dumps(receipt, indent=2))
    print(f"Receipt saved to {receipt_path}")

    print("\nSummary:")
    print(f"  Total tests: {total}")
    print(f"  Passed: {run['passed']}")
    print(f"  Failed: {run['failed']}")
    print(f"  Ignored: {run['ignored']}")

    if run['failed'] > 0:
        print(f"\n⚠️  {run['failed']} tests failed!")
        return 1

    print("\n✅ All executed tests passed")
    return 0

if __name__ == "__main__":
    exit(main())
