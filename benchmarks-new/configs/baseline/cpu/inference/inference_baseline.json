{
  "name": "Inference Engine Baseline (CPU)",
  "version": "1.0.0",
  "created": "2025-01-01T00:00:00Z",
  "git": {
    "sha": "baseline",
    "branch": "main",
    "tag": "v0.1.0"
  },
  "environment": {
    "device": "cpu",
    "arch": "x86_64",
    "simd": "avx2",
    "rust_version": "1.90.0",
    "model": "bitnet-b1.58-2B-4T"
  },
  "benchmarks": {
    "prefill_latency": {
      "seq_len_128": {
        "mean_ms": 45.2,
        "std_ms": 3.8,
        "tokens_per_second": 2832,
        "regression_threshold_percent": 20.0
      },
      "seq_len_256": {
        "mean_ms": 89.8,
        "std_ms": 7.2,
        "tokens_per_second": 2851,
        "regression_threshold_percent": 20.0
      },
      "seq_len_512": {
        "mean_ms": 179.4,
        "std_ms": 14.6,
        "tokens_per_second": 2854,
        "regression_threshold_percent": 20.0
      },
      "seq_len_1024": {
        "mean_ms": 358.9,
        "std_ms": 29.2,
        "tokens_per_second": 2853,
        "regression_threshold_percent": 20.0
      }
    },
    "decode_latency": {
      "single_token": {
        "mean_ms": 21.3,
        "std_ms": 1.8,
        "tokens_per_second": 46.9,
        "regression_threshold_percent": 15.0
      },
      "batch_4": {
        "mean_ms": 48.6,
        "std_ms": 4.1,
        "tokens_per_second": 82.3,
        "regression_threshold_percent": 15.0
      },
      "batch_8": {
        "mean_ms": 92.1,
        "std_ms": 7.8,
        "tokens_per_second": 86.9,
        "regression_threshold_percent": 15.0
      }
    },
    "end_to_end": {
      "prompt_32_gen_32": {
        "mean_ms": 1124.8,
        "std_ms": 89.2,
        "total_tokens_per_second": 56.9,
        "regression_threshold_percent": 25.0
      },
      "prompt_128_gen_128": {
        "mean_ms": 2798.4,
        "std_ms": 224.6,
        "total_tokens_per_second": 91.5,
        "regression_threshold_percent": 25.0
      }
    },
    "memory_usage": {
      "model_loading_mb": 1847.2,
      "peak_inference_mb": 2156.8,
      "cache_per_token_kb": 4.2,
      "regression_threshold_percent": 30.0
    }
  },
  "metadata": {
    "description": "Baseline performance metrics for inference engine on CPU",
    "notes": "End-to-end inference including tokenization, prefill, and decode phases",
    "test_iterations": 50,
    "warmup_iterations": 5
  }
}
